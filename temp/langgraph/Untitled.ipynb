{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef5c0d57-002b-4adc-a6e6-d606277fc89d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2739/424442926.py:4: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  ollama_agent = ChatOllama(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------research node-----------------\n",
      "## Ollama – A Quick‑start Guide to the “Local‑LLM” Ecosystem\n",
      "\n",
      "**What is Ollama?**  \n",
      "Ollama is a lightweight, open‑source framework that lets you download, run, and manage large‑language‑models (LLMs) on your own hardware—no cloud connection required. Think of it as a “Model Hub + Runtime” for LLMs that lives on your laptop, desktop, or even a small edge device. It was launched in 2023 by **Ollama, Inc.** (the same folks behind the popular *Ollama* chat app that runs locally) and has quickly become a go‑to solution for developers, researchers, and hobbyists who want privacy‑first, low‑latency AI without paying per‑token fees.\n",
      "\n",
      "---\n",
      "\n",
      "### 1. The Big Picture\n",
      "\n",
      "| Aspect | What Ollama Provides | Why It Matters |\n",
      "|--------|---------------------|----------------|\n",
      "| **Local inference** | Run any LLM on your GPU/CPU without a server | Keeps data on‑device, eliminates latency & cost |\n",
      "| **Model hub** | Pre‑built, ready‑to‑use weights (Llama 3, Gemma, Mistral, etc.) | No manual downloads or complicated setup |\n",
      "| **Unified API** | Simple CLI & HTTP REST interface | Quick prototyping & production‑grade integration |\n",
      "| **Cross‑platform** | Windows, macOS, Linux, Raspberry‑Pi, Android | Works on almost any machine |\n",
      "| **Open source** | All code (including the CLI, server, and web UI) is on GitHub | Inspect, modify, or contribute |\n",
      "\n",
      "---\n",
      "\n",
      "### 2. Core Components\n",
      "\n",
      "| Component | Role | Tech Stack |\n",
      "|-----------|------|------------|\n",
      "| **`ollama` CLI** | Pull models, run inference, manage local cache | Go (frontend), Rust (backend) |\n",
      "| **`ollama` daemon** | HTTP server exposing `/api/generate` and `/api/embeddings` | Go |\n",
      "| **Web UI** | Quick‑start chat interface (no code) | Vanilla JS/HTML/CSS |\n",
      "| **Python wrapper** | `ollama-python` library for programmatic access | Python 3.9+ |\n",
      "| **Community libraries** | e.g., `ollama-node`, `ollama-go` | Various languages |\n",
      "\n",
      "---\n",
      "\n",
      "### 3. How It Works Under the Hood\n",
      "\n",
      "1. **Model Storage**  \n",
      "   - Models are stored in a local folder (`~/.ollama/models/`).  \n",
      "   - Ollama uses the **ONNX**/`ggml` format for compact binary weights, enabling fast CPU inference when GPUs aren’t available.\n",
      "\n",
      "2. **Inference Engine**  \n",
      "   - GPU acceleration via CUDA (NVIDIA), Metal (Apple), or Vulkan/ROCm.  \n",
      "   - CPU fallback uses a highly optimized Rust engine that can run on single‑core machines.\n",
      "\n",
      "3. **Prompting**  \n",
      "   - Supports the same prompt templates used by OpenAI, making it trivial to swap between Ollama and the OpenAI API.  \n",
      "   - `--prompt` flag lets you pass text or a JSON prompt for advanced token control.\n",
      "\n",
      "4. **API**  \n",
      "   - `POST /api/generate` – stream or block text generation.  \n",
      "   - `POST /api/embeddings` – vector embeddings for semantic search.  \n",
      "   - `GET /api/models` – list available models locally.  \n",
      "\n",
      "---\n",
      "\n",
      "### 4. Using Ollama – A Minimal Example\n",
      "\n",
      "```bash\n",
      "# 1. Install (macOS example)\n",
      "brew install ollama\n",
      "\n",
      "# 2. Pull a model\n",
      "ollama pull llama3.1\n",
      "\n",
      "# 3. Run it locally\n",
      "ollama run llama3.1 \"Explain Ollama in 3 sentences.\"\n",
      "\n",
      "# 4. Using the HTTP API (curl)\n",
      "curl http://localhost:11434/api/generate \\\n",
      "  -H \"Content-Type: application/json\" \\\n",
      "  -d '{\"model\":\"llama3.1\",\"prompt\":\"What is Ollama?\",\"stream\":true}'\n",
      "```\n",
      "\n",
      "For developers, the Python library makes it even easier:\n",
      "\n",
      "```python\n",
      "import ollama\n",
      "\n",
      "# List local models\n",
      "print(ollama.list())\n",
      "\n",
      "# Generate text\n",
      "response = ollama.generate(\"llama3.1\", \"Describe the architecture of Ollama.\")\n",
      "print(response[\"response\"])\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### 5. Supported Models (as of 2025‑09)\n",
      "\n",
      "| Model | Origin | Size | Notes |\n",
      "|-------|--------|------|-------|\n",
      "| **Llama 3.1** | Meta | 8B–70B | Official weights, 8‑bit/16‑bit support |\n",
      "| **Gemma** | Google | 2B–8B | Lightweight, good for mobile |\n",
      "| **Mistral** | Mistral AI | 7B | Fast inference, open‑source weights |\n",
      "| **Phi‑2** | Microsoft | 2B | Very small, runs on even low‑end GPUs |\n",
      "| **Mixtral‑8x7B** | MosaicML | 8×7B | Multi‑head mixture‑of‑experts |\n",
      "| **Code Llama** | Meta | 7B/13B | Code‑specialized LLM |\n",
      "| **OpenAI compatible** | Custom | 3B+ | Some custom models mimic OpenAI APIs |\n",
      "\n",
      "> **Tip**: If you need a model that isn’t in the hub, you can upload your own ONNX or GGML file to `~/.ollama/models/`.\n",
      "\n",
      "---\n",
      "\n",
      "### 6. Use Cases\n",
      "\n",
      "| Category | Why Local Matters |\n",
      "|----------|-------------------|\n",
      "| **Privacy‑sensitive apps** | All data stays on the device (e.g., personal note‑taking, medical records). |\n",
      "| **Offline mode** | Work in areas without internet or on low‑bandwidth networks. |\n",
      "| **Cost‑efficiency** | No per‑token fees after initial download. |\n",
      "| **Rapid prototyping** | Spin up a model instantly, iterate locally, then deploy. |\n",
      "| **Research** | Full control over training data, model modifications, and debugging. |\n",
      "| **Edge devices** | Run LLM inference on Raspberry‑Pi, Jetson, or embedded systems. |\n",
      "\n",
      "---\n",
      "\n",
      "### 7. Community & Ecosystem\n",
      "\n",
      "| Resource | What It Offers |\n",
      "|----------|----------------|\n",
      "| **GitHub Repo** (`https://github.com/ollama/ollama`) | Source, issues, PRs. |\n",
      "| **Discord / Slack** | Community support, feature requests. |\n",
      "| **StackOverflow** | Tag `ollama` for Q&A. |\n",
      "| **Reddit** (`r/ollama`) | Announcements, tutorials. |\n",
      "| **Blogs & Tutorials** | e.g., *Ollama’s official blog*, *Hugging Face’s “Run LLM locally”* guide. |\n",
      "\n",
      "> **Contribution Note**: Ollama is actively maintained, with frequent releases. If you want to add a new model format or improve GPU support, the repo welcomes contributions.\n",
      "\n",
      "---\n",
      "\n",
      "### 8. Comparisons to Other LLM Platforms\n",
      "\n",
      "| Feature | Ollama | OpenAI API | LlamaIndex | Hugging Face Inference API |\n",
      "|---------|--------|------------|------------|----------------------------|\n",
      "| **Privacy** | Local only | Cloud | Depends on deployment | Cloud |\n",
      "| **Cost** | Zero after download | Token‑based | Depends on host | Token‑based |\n",
      "| **Latency** | Milliseconds (GPU) | 200‑500ms | Varies | 200‑500ms |\n",
      "| **Model Variety** | 30+ curated models | Few official models | Any LLM with LLMIndex | Any Hugging Face model |\n",
      "| **Ease of Setup** | 1‑click install | API key | Code + hosting | API key |\n",
      "| **Extensibility** | CLI + API + SDK | SDKs | SDKs | SDKs |\n",
      "| **Edge** | Yes | No | Yes (if self‑hosted) | No |\n",
      "\n",
      "---\n",
      "\n",
      "### 9. Potential Challenges & Mitigations\n",
      "\n",
      "| Challenge | How Ollama Addresses It |\n",
      "|-----------|------------------------|\n",
      "| **Large model size** | Uses compressed GGML/ONNX formats; supports quantization. |\n",
      "| **GPU requirements** | Works on CPU; offers 8‑bit quantization for low‑end GPUs. |\n",
      "| **Compatibility with new models** | Auto‑detects model metadata; you can create custom `manifest.json`. |\n",
      "| **Security** | No data leaves the device; still need to guard local storage. |\n",
      "| **License compliance** | Ollama doesn’t host proprietary weights; you must comply with each model’s license. |\n",
      "\n",
      "---\n",
      "\n",
      "### 10. Future Directions (What to Watch)\n",
      "\n",
      "1. **Model Hub Expansion** – Adding more fine‑tuned, specialized models (vision, multimodal).  \n",
      "2. **Containerized Deployment** – Docker images for easy server setup.  \n",
      "3. **Auto‑quantization tooling** – On‑the‑fly 4‑bit/8‑bit conversion for maximal efficiency.  \n",
      "4. **Federated Learning** – Collaborative training while keeping data local.  \n",
      "5. **Community‑driven UI** – More UI frameworks (React, Flutter) to embed chat widgets.  \n",
      "\n",
      "---\n",
      "\n",
      "## Quick Checklist for Getting Started\n",
      "\n",
      "1. **Install**: `brew install ollama` / `curl -fsSL https://ollama.com/install.sh | sh`  \n",
      "2. **Pull a Model**: `ollama pull llama3.1`  \n",
      "3. **Run**: `ollama run llama3.1 \"Hello world!\"`  \n",
      "4. **Use the API**: Point your app to `http://localhost:11434/api/generate`  \n",
      "5. **Optional**: Add `ollama` as a Python dependency and start coding!\n",
      "\n",
      "---\n",
      "\n",
      "### Final Thought\n",
      "\n",
      "Ollama redefines how we interact with large language models by **removing the middleman**. If you’re building a privacy‑first product, need low‑latency AI, or simply want to tinker with state‑of‑the‑art models without recurring costs, Ollama gives you a **drop‑in, open‑source, local‑first** solution that scales from a laptop to a small edge server.  \n",
      "\n",
      "Happy coding—your local AI assistant is just a CLI command away!\n",
      "-----------summary node-----------------\n",
      "**Ollama 速成精華三大重點**  \n",
      "\n",
      "1. **完全本地化、開源且輕量化**  \n",
      "   - 只需下載模型即可在自己的 GPU / CPU 上直接推理，無需雲端連線或付費使用。  \n",
      "   - 透過 `ollama` CLI、HTTP API 以及 Python 套件，提供統一且簡潔的操作介面。  \n",
      "\n",
      "2. **模型集市 + 即時推理引擎**  \n",
      "   - 內建多達 30+ 預訓練模型（Llama 3、Gemma、Mistral、Phi‑2 等），支援 ONNX/ggml 格式，便於快速安裝。  \n",
      "   - GPU 加速（CUDA、Metal、Vulkan/ROCm）與 CPU 最佳化引擎並行，可在桌面、筆電、Raspberry‑Pi、Android 等多平台運行。  \n",
      "\n",
      "3. **隱私、低延遲與成本優勢**  \n",
      "   - 所有資料皆留在本機，完全不需上傳雲端，適合隱私敏感或離線作業。  \n",
      "   - 初始下載後不再產生 token 費用，且推理延遲僅數毫秒（GPU）或秒級（CPU）。  \n",
      "   - 適用於快速原型開發、研究、邊緣裝置部署以及成本敏感的產品。\n",
      "-----------result-----------------\n",
      "**Ollama 速成精華三大重點**  \n",
      "\n",
      "1. **完全本地化、開源且輕量化**  \n",
      "   - 只需下載模型即可在自己的 GPU / CPU 上直接推理，無需雲端連線或付費使用。  \n",
      "   - 透過 `ollama` CLI、HTTP API 以及 Python 套件，提供統一且簡潔的操作介面。  \n",
      "\n",
      "2. **模型集市 + 即時推理引擎**  \n",
      "   - 內建多達 30+ 預訓練模型（Llama 3、Gemma、Mistral、Phi‑2 等），支援 ONNX/ggml 格式，便於快速安裝。  \n",
      "   - GPU 加速（CUDA、Metal、Vulkan/ROCm）與 CPU 最佳化引擎並行，可在桌面、筆電、Raspberry‑Pi、Android 等多平台運行。  \n",
      "\n",
      "3. **隱私、低延遲與成本優勢**  \n",
      "   - 所有資料皆留在本機，完全不需上傳雲端，適合隱私敏感或離線作業。  \n",
      "   - 初始下載後不再產生 token 費用，且推理延遲僅數毫秒（GPU）或秒級（CPU）。  \n",
      "   - 適用於快速原型開發、研究、邊緣裝置部署以及成本敏感的產品。\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from typing import TypedDict\n",
    "\n",
    "ollama_agent = ChatOllama(\n",
    "    model=\"gpt-oss:20b\",\n",
    "    base_url=\"http://10.1.1.59:11434\"\n",
    ")\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    query: str\n",
    "    research: str\n",
    "    summary: str\n",
    "# Agent A: Research Agent\n",
    "def research_node(state: State):\n",
    "    print(\"-----------research node-----------------\")\n",
    "    q = state[\"query\"]\n",
    "    res = ollama_agent.invoke(f\"Explain information about the following topic: {q}\")\n",
    "    state[\"research\"] = res.content\n",
    "    print(res.content)\n",
    "    return state\n",
    "\n",
    "# Agent B: Summarizer Agent\n",
    "def summary_node(state: State):\n",
    "    print(\"-----------summary node-----------------\")\n",
    "    text = state[\"research\"]\n",
    "    res = ollama_agent.invoke(f\"Use Traditional Chinese. Summarize the following content into three key points: {text}\")\n",
    "    state[\"summary\"] = res.content\n",
    "    print(res.content)\n",
    "    return state\n",
    "\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "graph = StateGraph(State)\n",
    "\n",
    "graph.add_node(\"ResearchAgent\", research_node)\n",
    "graph.add_node(\"SummarizerAgent\", summary_node)\n",
    "\n",
    "graph.set_entry_point(\"ResearchAgent\")\n",
    "graph.add_edge(\"ResearchAgent\", \"SummarizerAgent\")\n",
    "graph.add_edge(\"SummarizerAgent\", END)\n",
    "\n",
    "app = graph.compile()\n",
    "\n",
    "resp = app.invoke({\"query\": \"Ollama\"})\n",
    "\n",
    "print(\"-----------result-----------------\")\n",
    "print(resp[\"summary\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e7c9379-cbcb-4de6-a005-a59fc9da10c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Go 裡的 := 是什麼意思？\n",
      "→ 分配給: {\n",
      "  \"agent\": \"SyntaxCoachAgent\"\n",
      "}\n",
      "\n",
      "Q: 幫我寫一個 Go REST API 範例\n",
      "→ 分配給: {\n",
      "  \"agent\": \"PracticalExampleAgent\"\n",
      "}\n",
      "\n",
      "Q: 我安裝 Go 之後執行 go run 報錯\n",
      "→ 分配給: {\n",
      "  \"agent\": \"TroubleshootingAgent\"\n",
      "}\n",
      "\n",
      "Q: 請幫我規劃一份學習 Golang 的知識架構\n",
      "→ 分配給: {\n",
      "  \"agent\": \"KnowledgeTreeAgent\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from typing import TypedDict\n",
    "\n",
    "ollama_agent = ChatOllama(\n",
    "    model=\"gpt-oss:20b\",\n",
    "    base_url=\"http://10.1.1.59:11434\"\n",
    ")\n",
    "\n",
    "ROUTER_PROMPT = \"\"\"\n",
    "你是一個問題分流 Agent。  \n",
    "你的任務是根據使用者的輸入，判斷應該由哪個專家 Agent 處理。  \n",
    "請只輸出對應的 Agent 名稱，不要輸出多餘文字。  \n",
    "\n",
    "可選的 Agent 類別有：  \n",
    "- TutorialAgent → 安裝、環境設定、入門指引  \n",
    "- SyntaxCoachAgent → Go 語法、資料型態、關鍵字解釋（例如 :=, defer, map, struct, interface）  \n",
    "- PracticalExampleAgent → REST API、測試、範例程式碼、模組應用  \n",
    "- ConcurrencyExpertAgent → goroutines、channels、select、同步/非同步程式設計  \n",
    "- TroubleshootingAgent → 錯誤訊息診斷、編譯問題、模組管理錯誤  \n",
    "- KnowledgeTreeAgent → 系統化知識結構、學習路徑、知識樹  \n",
    "\n",
    "輸出格式（JSON）：  \n",
    "{\n",
    "  \"agent\": \"<AgentName>\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def route_question(user_question: str) -> str:\n",
    "    \"\"\"RouterAgent: 分派使用者的問題到對應的 Agent\"\"\"\n",
    "    \n",
    "    prompt = ROUTER_PROMPT + f\"\\n\\n使用者的輸入是：{user_question}\"\n",
    "    # print(prompt)\n",
    "\n",
    "    response = ollama_agent.invoke(prompt)   # 用 langchain_community ChatOllama\n",
    "    output = response.content.strip()\n",
    "\n",
    "    return output\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_questions = [\n",
    "        \"Go 裡的 := 是什麼意思？\",\n",
    "        \"幫我寫一個 Go REST API 範例\",\n",
    "        \"我安裝 Go 之後執行 go run 報錯\",\n",
    "        \"請幫我規劃一份學習 Golang 的知識架構\",\n",
    "    ]\n",
    "\n",
    "    for q in test_questions:\n",
    "        agent = route_question(q)\n",
    "        print(f\"Q: {q}\\n→ 分配給: {agent}\\n\")\n",
    "# graph = StateGraph()\n",
    "\n",
    "# # 節點定義\n",
    "# graph.add_node(\"RouterAgent\", router_agent)\n",
    "# graph.add_node(\"TutorialAgent\", tutorial_agent)\n",
    "# graph.add_node(\"SyntaxCoachAgent\", syntax_coach_agent)\n",
    "# graph.add_node(\"PracticalExampleAgent\", practical_example_agent)\n",
    "# graph.add_node(\"ConcurrencyExpertAgent\", concurrency_expert_agent)\n",
    "# graph.add_node(\"TroubleshootingAgent\", troubleshooting_agent)\n",
    "# graph.add_node(\"KnowledgeTreeAgent\", knowledge_tree_agent)\n",
    "# graph.add_node(\"AnswerCollector\", answer_collector)\n",
    "\n",
    "# # 邊 (Router → 對應 Agent)\n",
    "# graph.add_edge(\"RouterAgent\", \"TutorialAgent\")\n",
    "# graph.add_edge(\"RouterAgent\", \"SyntaxCoachAgent\")\n",
    "# graph.add_edge(\"RouterAgent\", \"PracticalExampleAgent\")\n",
    "# graph.add_edge(\"RouterAgent\", \"ConcurrencyExpertAgent\")\n",
    "# graph.add_edge(\"RouterAgent\", \"TroubleshootingAgent\")\n",
    "# graph.add_edge(\"RouterAgent\", \"KnowledgeTreeAgent\")\n",
    "\n",
    "# # Agent → AnswerCollector\n",
    "# for agent in [\n",
    "#     \"TutorialAgent\", \"SyntaxCoachAgent\", \"PracticalExampleAgent\",\n",
    "#     \"ConcurrencyExpertAgent\", \"TroubleshootingAgent\", \"KnowledgeTreeAgent\"\n",
    "# ]:\n",
    "#     graph.add_edge(agent, \"AnswerCollector\")\n",
    "\n",
    "# # AnswerCollector → END\n",
    "# graph.add_edge(\"AnswerCollector\", END)\n",
    "\n",
    "# # 入口\n",
    "# graph.set_entry_point(\"RouterAgent\")\n",
    "\n",
    "# # 建立應用\n",
    "# app = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3323e2bd-0877-4dc9-b596-4919e7289d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RouterAgent + 4 Agents (Ollama) ===\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You >  run vscode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================route_question\n",
      "content='DevEnvAgent' additional_kwargs={} response_metadata={'model': 'gpt-oss:20b', 'created_at': '2025-09-14T13:17:31.399820097Z', 'done': True, 'done_reason': 'stop', 'total_duration': 619140148, 'load_duration': 82524262, 'prompt_eval_count': 243, 'prompt_eval_duration': 29764787, 'eval_count': 33, 'eval_duration': 504725680, 'model_name': 'gpt-oss:20b'} id='run--9dfbca75-e637-4861-9823-d5a044618965-0' usage_metadata={'input_tokens': 243, 'output_tokens': 33, 'total_tokens': 276}\n",
      "=================route_question\n",
      "DevEnvAgent\n",
      "[Router → DevEnvAgent]\n",
      "以下是使用 VSCode 來啟動並除錯 Moby 專案的完整流程（包含 Dev Container 與 Debug 模式）：\n",
      "\n",
      "---\n",
      "\n",
      "## 1️⃣ 先決條件\n",
      "\n",
      "| 項目 | 需求 | 參考 |\n",
      "|------|------|------|\n",
      "| VSCode | 版本 1.70+ |  |\n",
      "| VSCode 插件 | **Dev Containers** |  |\n",
      "| Docker | 安裝在宿主機（VM）上 |  |\n",
      "| 目錄 | 你已經 clone 下來的 `moby` 專案 |  |\n",
      "\n",
      "> **提示**：如果你還沒安裝 Docker，請先在宿主機上安裝並確認 `docker run hello-world` 能正常執行。\n",
      "\n",
      "---\n",
      "\n",
      "## 2️⃣ 在 VSCode 內部啟動 Dev Container\n",
      "\n",
      "1. **打開 VSCode**  \n",
      "   - 進入你 clone 下來的 `moby` 專案資料夾。\n",
      "\n",
      "2. **安裝 Dev Containers 插件**  \n",
      "   - 在左側擴充功能面板搜尋 `Dev Containers`，點擊安裝。\n",
      "\n",
      "3. **重啟 VSCode**  \n",
      "   - 安裝完成後，VSCode 會提示你重啟，請按下 **Reload**。\n",
      "\n",
      "4. **開啟 Dev Container**  \n",
      "   - 在 VSCode 命令面板（`Ctrl+Shift+P`）輸入 `Dev Containers: Reopen in Container`，選擇它。  \n",
      "   - VSCode 會自動下載並啟動容器，並在容器內顯示一個 Bash 终端。\n",
      "\n",
      "---\n",
      "\n",
      "## 3️⃣ 設定 Debug 環境 (`launch.json`)\n",
      "\n",
      "1. 在 VSCode 左側的 **Run** 面板（或按 `Ctrl+Shift+D`）點擊 **create a launch.json file**。  \n",
      "2. 選擇 **Go**，然後貼上以下設定：\n",
      "\n",
      "   ```json\n",
      "   {\n",
      "       \"version\": \"0.2.0\",\n",
      "       \"configurations\": [\n",
      "           {\n",
      "               \"name\": \"Launch Package\",\n",
      "               \"type\": \"go\",\n",
      "               \"request\": \"launch\",\n",
      "               \"mode\": \"auto\",\n",
      "               \"program\": \"${fileDirname}\"\n",
      "           }\n",
      "       ]\n",
      "   }\n",
      "   ```\n",
      "\n",
      "3. 這樣就可以在容器內直接以 Go 語言的方式除錯任何 Go 檔案。\n",
      "\n",
      "---\n",
      "\n",
      "## 4️⃣ 常見錯誤與解決方案\n",
      "\n",
      "| 錯誤 | 可能原因 | 解決方法 |\n",
      "|------|----------|----------|\n",
      "| `error: Invalid userlane-proxy-path: userland-proxy is enabled, but userland-proxy-path is not set` | Docker daemon 設定缺少 `userland-proxy-path` | 在 `/etc/docker/daemon.json` 加入 `\"userland-proxy\": false` 或設定正確路徑 |\n",
      "| `Running modprobe bridge br_netfilter failed with message: \" error=\"exec: \\\"modprobe\\\": executable file not found in $PATH` | 容器內缺少 `modprobe` 或 `iptables` | 重新安裝 iptables：`apt remove iptables && apt install iptables` |\n",
      "| `docker swarm init` 失敗 | 防火牆阻擋 2377 端口 | 在 Rocky Linux 8 VM 上停用 firewalld：`systemctl stop firewalld && systemctl disable firewalld` |\n",
      "\n",
      "---\n",
      "\n",
      "## 5️⃣ 進階：在 VSCode 內部啟動 Docker Swarm\n",
      "\n",
      "1. **停用宿主機防火牆**（若使用 Rocky Linux 8）  \n",
      "   ```bash\n",
      "   systemctl stop firewalld\n",
      "   systemctl disable firewalld\n",
      "   ```\n",
      "\n",
      "2. **在 VSCode Dev Container 內部執行**  \n",
      "   ```bash\n",
      "   docker swarm init --advertise-addr <your-container-ip>\n",
      "   ```\n",
      "\n",
      "3. **確認 Swarm 端口**  \n",
      "   ```bash\n",
      "   netstat -anp | grep :2377\n",
      "   ```\n",
      "\n",
      "4. **在宿主機（或另一台 VM）加入 Swarm**  \n",
      "   ```bash\n",
      "   docker swarm join --token <token> <manager-ip>:2377\n",
      "   ```\n",
      "\n",
      "5. **在 VSCode 內部確認**  \n",
      "   - 透過 `docker node ls` 或 `docker service ls` 查看節點與服務。\n",
      "\n",
      "---\n",
      "\n",
      "## 6️⃣ 小結\n",
      "\n",
      "1. **安裝 VSCode + Dev Containers**  \n",
      "2. **Reopen in Container** → 進入容器 Bash  \n",
      "3. **設定 `launch.json`** → 開始除錯  \n",
      "4. **處理常見錯誤**（userland‑proxy、iptables、firewalld）  \n",
      "5. **可選：啟動 Docker Swarm** → 進行多節點測試  \n",
      "\n",
      "只要按上述步驟操作，你就能在 VSCode 內部順利啟動、除錯並測試 Moby 專案。祝開發順利 🚀\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You >  如何填寫launch.json in vscode, launch.json only\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================route_question\n",
      "content='DevEnvAgent' additional_kwargs={} response_metadata={'model': 'gpt-oss:20b', 'created_at': '2025-09-14T13:18:33.836950809Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1268998446, 'load_duration': 82414253, 'prompt_eval_count': 252, 'prompt_eval_duration': 126462759, 'eval_count': 68, 'eval_duration': 1058099943, 'model_name': 'gpt-oss:20b'} id='run--36f1b7c0-8367-4b68-8331-02801b8a086f-0' usage_metadata={'input_tokens': 252, 'output_tokens': 68, 'total_tokens': 320}\n",
      "=================route_question\n",
      "DevEnvAgent\n",
      "[Router → DevEnvAgent]\n",
      "```json\n",
      "{\n",
      "    \"version\": \"0.2.0\",\n",
      "    \"configurations\": [\n",
      "        {\n",
      "            \"name\": \"Launch Package\",\n",
      "            \"type\": \"go\",\n",
      "            \"request\": \"launch\",\n",
      "            \"mode\": \"auto\",\n",
      "            \"program\": \"${fileDirname}\"\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "```\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You >  how to run a unit test\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================route_question\n",
      "content='TestingAgent' additional_kwargs={} response_metadata={'model': 'gpt-oss:20b', 'created_at': '2025-09-14T13:19:20.096410663Z', 'done': True, 'done_reason': 'stop', 'total_duration': 647488399, 'load_duration': 81606237, 'prompt_eval_count': 247, 'prompt_eval_duration': 123419498, 'eval_count': 29, 'eval_duration': 440470106, 'model_name': 'gpt-oss:20b'} id='run--10e6eec3-c24a-4930-a9d0-e4a33ea363b8-0' usage_metadata={'input_tokens': 247, 'output_tokens': 29, 'total_tokens': 276}\n",
      "=================route_question\n",
      "TestingAgent\n",
      "[Router → TestingAgent]\n",
      "要執行單元測試，請依照 **TESTING.md** 中的說明：\n",
      "\n",
      "1. 在專案根目錄執行  \n",
      "   ```bash\n",
      "   make test-unit\n",
      "   ```\n",
      "2. 測試結果會顯示在 **TEST RESULT** 面板中。  \n",
      "\n",
      "這就是執行單元測試的完整流程。\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You >  test有幾種種類？\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================route_question\n",
      "content='TestingAgent' additional_kwargs={} response_metadata={'model': 'gpt-oss:20b', 'created_at': '2025-09-14T13:19:41.355620904Z', 'done': True, 'done_reason': 'stop', 'total_duration': 884866037, 'load_duration': 83092685, 'prompt_eval_count': 248, 'prompt_eval_duration': 121132874, 'eval_count': 44, 'eval_duration': 678632751, 'model_name': 'gpt-oss:20b'} id='run--80086df1-f5c6-42e8-a812-bbab062f7b48-0' usage_metadata={'input_tokens': 248, 'output_tokens': 44, 'total_tokens': 292}\n",
      "=================route_question\n",
      "TestingAgent\n",
      "[Router → TestingAgent]\n",
      "在 Moby Project 的測試流程中，主要分為 **兩種測試類型**：\n",
      "\n",
      "| 類型 | 主要用途 | 相關指令或設定 |\n",
      "|------|----------|---------------|\n",
      "| **單元測試 (Unit Test)** | 驗證單一模組或函式的正確性 | `make test-unit` |\n",
      "| **整合測試 (Integration Test)** | 驗證多個模組之間的協同工作，或整體功能的正確性 | `make test-integration`（需先設定容器、安裝 iptables 等） |\n",
      "\n",
      "> 除了上述兩大類型，文件中還說明了如何在 Debug 模式下執行單一單元測試，以及如何針對特定整合測試案例（如 `TestBridgeICC/IPv4_non-internal_network`）進行單一整合測試，但這些仍屬於「單元測試」或「整合測試」的範疇。  \n",
      "\n",
      "因此，Moby Project 的測試方式總共可歸納為 **兩種主要類型**：單元測試和整合測試。\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You >  整合測試 (Integration Test)*步驟\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================route_question\n",
      "content='TestingAgent' additional_kwargs={} response_metadata={'model': 'gpt-oss:20b', 'created_at': '2025-09-14T13:22:20.483600577Z', 'done': True, 'done_reason': 'stop', 'total_duration': 963272972, 'load_duration': 78978968, 'prompt_eval_count': 252, 'prompt_eval_duration': 124372806, 'eval_count': 49, 'eval_duration': 757885207, 'model_name': 'gpt-oss:20b'} id='run--f19b12a0-075e-4eea-aa75-8fac4983bac5-0' usage_metadata={'input_tokens': 252, 'output_tokens': 49, 'total_tokens': 301}\n",
      "=================route_question\n",
      "TestingAgent\n",
      "[Router → TestingAgent]\n",
      "**整合測試（Integration Test）步驟**\n",
      "\n",
      "| 步驟 | 操作說明 | 相關指令 / 參數 |\n",
      "|------|----------|-----------------|\n",
      "| 1 | **建立容器並執行 Docker Daemon** | 先在測試環境中啟動一個容器，並在容器內執行 `dockerd`。 |\n",
      "| 2 | **修改 `hack/make.sh`** | 在 `hack/make.sh` 裡新增以下兩行，確保 iptables 版本正確：<br>```bash<br>apt remove iptables -y<br>apt install iptables -y<br>``` |\n",
      "| 3 | **執行整合測試** | ```bash<br>make test-integration<br>``` |\n",
      "| 4 | **（單一測試）設定環境變數** | 若只想跑單一測試，先設定環境變數：<br>```bash<br>export TESTFLAGS='-test.run TestBridgeICC/IPv4_non-internal_network'<br>export TEST_INTEGRATION_DIR=./integration/networking<br>```<br>然後再執行 `make test-integration`。 |\n",
      "| 5 | **查看測試結果** | 測試結果會顯示在 **TEST RESULT** 面板。<br>（參考附圖：`image.png`、`scaled-1680-/JuBimage.png` 等） |\n",
      "\n",
      "> **備註**  \n",
      "> - 步驟 2 的 iptables 重新安裝是為了避免測試環境中 iptables 版本不一致造成的錯誤。  \n",
      "> - 步驟 4 的環境變數 `TESTFLAGS` 允許你只跑指定的測試函式，`TEST_INTEGRATION_DIR` 指定測試目錄。  \n",
      "> - 所有測試結果均會在 **TEST RESULT** 面板中呈現，並且文件中已附上相應的截圖供參考。\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You >  我想要跑moby project, 用哪種ide好？\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================route_question\n",
      "content='DevEnvAgent' additional_kwargs={} response_metadata={'model': 'gpt-oss:20b', 'created_at': '2025-09-14T13:24:07.191555624Z', 'done': True, 'done_reason': 'stop', 'total_duration': 835486441, 'load_duration': 81871387, 'prompt_eval_count': 255, 'prompt_eval_duration': 121337921, 'eval_count': 41, 'eval_duration': 630336970, 'model_name': 'gpt-oss:20b'} id='run--f5d0490f-c849-44ef-9b80-894862349bdf-0' usage_metadata={'input_tokens': 255, 'output_tokens': 41, 'total_tokens': 296}\n",
      "=================route_question\n",
      "DevEnvAgent\n",
      "[Router → DevEnvAgent]\n",
      "根據文件中的說明，**VSCode（搭配 Dev Containers 擴充套件）** 是跑 Moby Project 最常用且最完整的開發環境。  \n",
      "\n",
      "### 為什麼選擇 VSCode + Dev Containers？\n",
      "\n",
      "| 特色 | 說明 |\n",
      "|------|------|\n",
      "| **容器化開發環境** | Dev Containers 讓你在 VSCode 內直接進入 Moby 的開發容器，環境與 CI 一致。 |\n",
      "| **即時 Debug** | 只要在 `launch.json` 設定好 `go` 調試配置，即可在容器內直接斷點、單步執行。 |\n",
      "| **完整的 Docker 支援** | VSCode 內建 Docker 擴充套件，可直接管理容器、鏡像、Swarm。 |\n",
      "| **跨平台** | 無論你在 Windows、macOS 或 Linux 上，都能以相同方式開發。 |\n",
      "\n",
      "### 快速上手流程（參考文件）\n",
      "\n",
      "1. **安裝 VSCode**  \n",
      "   - 下載並安裝 VSCode。  \n",
      "2. **安裝 Dev Containers 擴充套件**  \n",
      "   - 在 VSCode Marketplace 搜尋 `Dev Containers`，點擊安裝。  \n",
      "3. **重啟 VSCode**  \n",
      "   - 重新啟動後，VSCode 會顯示「container bash」提示。  \n",
      "4. **Reopen in Container**  \n",
      "   - 在 VSCode 指令面板（`Ctrl+Shift+P`）輸入 `Reopen in Container`，讓專案在容器內開啟。  \n",
      "5. **設定 `launch.json`**  \n",
      "   ```json\n",
      "   {\n",
      "       \"version\": \"0.2.0\",\n",
      "       \"configurations\": [\n",
      "           {\n",
      "               \"name\": \"Launch Package\",\n",
      "               \"type\": \"go\",\n",
      "               \"request\": \"launch\",\n",
      "               \"mode\": \"auto\",\n",
      "               \"program\": \"${fileDirname}\"\n",
      "           }\n",
      "       ]\n",
      "   }\n",
      "   ```\n",
      "6. **解決常見錯誤**  \n",
      "   - 若出現 `error: Invalid userlane-proxy-path`，請在 `/etc/docker/daemon.json` 設定 `userland-proxy: false`。  \n",
      "   - 若出現 `modprobe bridge br_netfilter failed`，請重新安裝 `iptables`：  \n",
      "     ```bash\n",
      "     apt remove iptables\n",
      "     apt install iptables\n",
      "     ```\n",
      "7. **執行 Debug**  \n",
      "   - 在 VSCode 中按下 `F5` 或點擊「Run」按鈕，即可進入 Debug 模式。  \n",
      "\n",
      "> **備註**  \n",
      "> - 若你想在 Dev Container 內跑完整的 Docker Engine（`dockerd`），請參考「Run Moby inside Dev Container」的步驟：  \n",
      ">   ```bash\n",
      ">   hack/make.sh binary\n",
      ">   make install\n",
      ">   dockerd -D\n",
      ">   ```  \n",
      "> - 若你想在 VSCode 內啟動 Docker Swarm Manager，請參考「Run Moby Project and Create Docker Swarm Manager in VSCode」的說明。  \n",
      "\n",
      "### 結論\n",
      "\n",
      "- **最佳選擇**：VSCode + Dev Containers  \n",
      "- **優點**：容器化一致環境、即時 Debug、跨平台、豐富擴充套件。  \n",
      "\n",
      "如果你已經熟悉 VSCode，直接使用上述流程即可快速開始開發 Moby Project。祝開發順利！\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You >  error: Invalid userlane-proxy-path 這個是哪個agent?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================route_question\n",
      "content='IssueAgent' additional_kwargs={} response_metadata={'model': 'gpt-oss:20b', 'created_at': '2025-09-14T13:25:11.132298002Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1668767150, 'load_duration': 82351201, 'prompt_eval_count': 256, 'prompt_eval_duration': 120405838, 'eval_count': 94, 'eval_duration': 1463940173, 'model_name': 'gpt-oss:20b'} id='run--4764f7fd-c523-456e-a81e-d76a68679c7f-0' usage_metadata={'input_tokens': 256, 'output_tokens': 94, 'total_tokens': 350}\n",
      "=================route_question\n",
      "IssueAgent\n",
      "[Router → IssueAgent]\n",
      "抱歉，根據目前提供的文件內容，我找不到任何關於「Invalid userlane-proxy-path」的 Issue 或相關 agent。若您有其他相關資訊或想查詢其他已列出的 Issue，請隨時告訴我。\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You >  開發的時後碰到error: Invalid userlane-proxy-path \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================route_question\n",
      "content='DevEnvAgent' additional_kwargs={} response_metadata={'model': 'gpt-oss:20b', 'created_at': '2025-09-14T13:25:39.86829392Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2175119468, 'load_duration': 84099209, 'prompt_eval_count': 256, 'prompt_eval_duration': 120962317, 'eval_count': 126, 'eval_duration': 1967920663, 'model_name': 'gpt-oss:20b'} id='run--2623a3ec-4292-42dc-bba4-4bbf5f9ce305-0' usage_metadata={'input_tokens': 256, 'output_tokens': 126, 'total_tokens': 382}\n",
      "=================route_question\n",
      "DevEnvAgent\n",
      "[Router → DevEnvAgent]\n",
      "**解決「error: Invalid userlane‑proxy‑path」的步驟**\n",
      "\n",
      "這個錯誤訊息表示 Docker 的 `userland-proxy` 功能已經啟用，但系統找不到對應的執行檔路徑。  \n",
      "在 Moby 的開發環境中，最常見的做法是 **關閉** `userland-proxy`，或是明確指定其路徑。\n",
      "\n",
      "---\n",
      "\n",
      "### 1. 先確認 `/etc/docker/daemon.json`\n",
      "\n",
      "```bash\n",
      "sudo nano /etc/docker/daemon.json\n",
      "```\n",
      "\n",
      "如果檔案不存在，先建立一個。  \n",
      "在檔案中加入（或確認已存在）以下設定：\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"runtimes\": {\n",
      "        \"crun\": {\n",
      "            \"path\": \"/usr/local/bin/crun\"\n",
      "        }\n",
      "    },\n",
      "    \"features\": {\n",
      "        \"containerd-snapshotter\": false\n",
      "    },\n",
      "    \"userland-proxy\": false,\n",
      "    \"iptables\": false\n",
      "}\n",
      "```\n",
      "\n",
      "> **說明**  \n",
      "> - `userland-proxy: false` 會關閉該功能，避免需要指定路徑。  \n",
      "> - `iptables: false` 也常在開發環境中關閉，以減少額外的網路設定。\n",
      "\n",
      "> 如果你想保留 `userland-proxy`，可以改成  \n",
      "> ```json\n",
      "> \"userland-proxy-path\": \"/usr/local/bin/userland-proxy\"\n",
      "> ```  \n",
      "> 但這需要你確定該執行檔已安裝在該路徑。\n",
      "\n",
      "---\n",
      "\n",
      "### 2. 重新啟動 Docker\n",
      "\n",
      "```bash\n",
      "sudo systemctl restart docker\n",
      "```\n",
      "\n",
      "或在 Dev Container 內：\n",
      "\n",
      "```bash\n",
      "dockerd -D\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### 3. 驗證\n",
      "\n",
      "```bash\n",
      "docker info | grep -i userland\n",
      "```\n",
      "\n",
      "應該顯示 `Userland Proxy: false`（或已關閉）。\n",
      "\n",
      "---\n",
      "\n",
      "### 4. 其他可能的錯誤\n",
      "\n",
      "- **iptables 未安裝**  \n",
      "  若還出現 `Running modprobe bridge br_netfilter failed`，請先重新安裝 iptables：\n",
      "\n",
      "  ```bash\n",
      "  sudo apt remove iptables\n",
      "  sudo apt install iptables\n",
      "  ```\n",
      "\n",
      "- **確保 VSCode 的 Dev Containers 已正確啟動**  \n",
      "  重新開啟容器：`Reopen in Container`，並確認 `launch.json` 的設定正確。\n",
      "\n",
      "---\n",
      "\n",
      "#### 小結\n",
      "\n",
      "- **關閉** `userland-proxy`（最簡單）或  \n",
      "- **指定** `userland-proxy-path`（若必須使用）。\n",
      "\n",
      "完成上述步驟後，開發環境中的 `error: Invalid userlane-proxy-path` 應該就會消失。祝開發順利！\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You >  code trace : VSCode 返回上一個檢視的快捷鍵\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================route_question\n",
      "content='DevEnvAgent' additional_kwargs={} response_metadata={'model': 'gpt-oss:20b', 'created_at': '2025-09-14T13:30:13.129131153Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1399012130, 'load_duration': 91811583, 'prompt_eval_count': 255, 'prompt_eval_duration': 120832930, 'eval_count': 76, 'eval_duration': 1184357692, 'model_name': 'gpt-oss:20b'} id='run--6233bc4d-0b3d-4f47-b381-42ac47c0d53d-0' usage_metadata={'input_tokens': 255, 'output_tokens': 76, 'total_tokens': 331}\n",
      "=================route_question\n",
      "DevEnvAgent\n",
      "[Router → DevEnvAgent]\n",
      "抱歉，我目前只能根據您提供的文件內容回答問題。文件中並未包含 VSCode 的快捷鍵資訊，因此無法提供「返回上一個檢視」的快捷鍵。若您需要此資訊，建議查閱 VSCode 官方文件或設定頁面。\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You >  開發的時候遇到 error: Invalid userlane-proxy-path: userland-proxy is enabled, but userland-proxy-path is not set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================route_question\n",
      "content='DevEnvAgent' additional_kwargs={} response_metadata={'model': 'gpt-oss:20b', 'created_at': '2025-09-14T13:41:40.938459643Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2897454124, 'load_duration': 77779212, 'prompt_eval_count': 273, 'prompt_eval_duration': 145257615, 'eval_count': 171, 'eval_duration': 2671951788, 'model_name': 'gpt-oss:20b'} id='run--63c35fe2-cc97-4e32-bb98-a9a86a8e9059-0' usage_metadata={'input_tokens': 273, 'output_tokens': 171, 'total_tokens': 444}\n",
      "=================route_question\n",
      "DevEnvAgent\n",
      "[Router → DevEnvAgent]\n",
      "**解決「Invalid userlane‑proxy‑path: userland‑proxy is enabled, but userland‑proxy‑path is not set」的步驟**\n",
      "\n",
      "這個錯誤是因為 Docker 的 `userland-proxy` 功能被啟用，但沒有指定對應的執行檔路徑。最簡單的做法是 **關閉** `userland-proxy`，或是明確指定路徑。以下是兩種常見的修正方式，請依照你目前的開發環境選擇其中一種：\n",
      "\n",
      "---\n",
      "\n",
      "### 1. 直接關閉 `userland-proxy`\n",
      "\n",
      "1. **編輯** `/etc/docker/daemon.json`（如果檔案不存在，先建立一個）  \n",
      "   ```bash\n",
      "   sudo nano /etc/docker/daemon.json\n",
      "   ```\n",
      "\n",
      "2. 在 JSON 內容中加入或修改以下設定：\n",
      "   ```json\n",
      "   {\n",
      "       \"userland-proxy\": false,\n",
      "       \"iptables\": false\n",
      "   }\n",
      "   ```\n",
      "   > *說明*  \n",
      "   * `userland-proxy: false` 會把這個功能關閉，避免 Docker 需要 `userland-proxy-path`。  \n",
      "   * `iptables: false` 也常與此設定一起使用，避免在容器內部再啟用 iptables。\n",
      "\n",
      "3. **重啟 Docker** 以套用設定：\n",
      "   ```bash\n",
      "   sudo systemctl restart docker\n",
      "   ```\n",
      "\n",
      "4. 再次執行你之前的指令，錯誤應該已消失。\n",
      "\n",
      "---\n",
      "\n",
      "### 2. 指定 `userland-proxy-path`\n",
      "\n",
      "如果你需要保留 `userland-proxy`（例如某些舊版容器需要它），可以直接告訴 Docker 它的執行檔路徑：\n",
      "\n",
      "1. **確認** `userland-proxy` 可執行檔位置（通常在 `/usr/local/bin/userland-proxy` 或 `/usr/bin/userland-proxy`）。  \n",
      "   ```bash\n",
      "   which userland-proxy\n",
      "   ```\n",
      "\n",
      "2. **編輯** `/etc/docker/daemon.json`，加入 `userland-proxy-path`：\n",
      "   ```json\n",
      "   {\n",
      "       \"userland-proxy\": true,\n",
      "       \"userland-proxy-path\": \"/usr/local/bin/userland-proxy\",\n",
      "       \"iptables\": false\n",
      "   }\n",
      "   ```\n",
      "\n",
      "3. **重啟 Docker**：\n",
      "   ```bash\n",
      "   sudo systemctl restart docker\n",
      "   ```\n",
      "\n",
      "4. 再次執行你的程式，錯誤應該已解決。\n",
      "\n",
      "---\n",
      "\n",
      "### 3. 在 VSCode Dev Container 內部\n",
      "\n",
      "如果你是在 VSCode 的 Dev Container 內部遇到此錯誤，請確保：\n",
      "\n",
      "1. **容器內部**也有相同的 `/etc/docker/daemon.json` 設定。  \n",
      "   - 你可以在容器的 `bash` 內部直接編輯 `/etc/docker/daemon.json`，或在 `devcontainer.json` 的 `postCreateCommand` 裡面寫入設定檔。\n",
      "\n",
      "2. **重啟容器內部的 Docker**（如果你在容器內部跑 `dockerd`）：\n",
      "   ```bash\n",
      "   dockerd -D\n",
      "   ```\n",
      "\n",
      "3. **確認 iptables 已安裝**（若你使用 `iptables: false`，仍需安裝 iptables 以避免其他錯誤）：\n",
      "   ```bash\n",
      "   apt remove iptables\n",
      "   apt install iptables\n",
      "   ```\n",
      "\n",
      "---\n",
      "\n",
      "### 小結\n",
      "\n",
      "| 方案 | 何時使用 | 主要設定 |\n",
      "|------|----------|----------|\n",
      "| 關閉 `userland-proxy` | 你不需要此功能 | `\"userland-proxy\": false` |\n",
      "| 指定 `userland-proxy-path` | 你需要保留此功能 | `\"userland-proxy-path\": \"/path/to/userland-proxy\"` |\n",
      "\n",
      "只要把上述設定寫進 `/etc/docker/daemon.json`，重啟 Docker（或容器內部的 `dockerd`），就能消除「Invalid userlane‑proxy‑path」的錯誤。祝開發順利！\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You >  我想要跑debug mode moby project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================route_question\n",
      "content='DevEnvAgent' additional_kwargs={} response_metadata={'model': 'gpt-oss:20b', 'created_at': '2025-09-14T13:43:18.405644041Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1579931006, 'load_duration': 83032606, 'prompt_eval_count': 250, 'prompt_eval_duration': 122727509, 'eval_count': 88, 'eval_duration': 1371472302, 'model_name': 'gpt-oss:20b'} id='run--0c576023-da99-42cd-9c4a-e83386c53b2e-0' usage_metadata={'input_tokens': 250, 'output_tokens': 88, 'total_tokens': 338}\n",
      "=================route_question\n",
      "DevEnvAgent\n",
      "[Router → DevEnvAgent]\n",
      "以下是你可以直接照著執行的步驟，讓 Moby Project 在 VSCode 內以 **debug mode** 執行。  \n",
      "（所有資訊均來自你提供的文件，請依照順序操作，若遇到錯誤請參考對應的「可能錯誤」說明。）\n",
      "\n",
      "---\n",
      "\n",
      "## 1️⃣ 先確定開發環境已安裝好\n",
      "\n",
      "| 步驟 | 內容 | 參考 |\n",
      "|------|------|------|\n",
      "| 1 | **安裝 VSCode**（若尚未安裝） |  |\n",
      "| 2 | **安裝 VSCode 插件：Dev Containers** |  |\n",
      "| 3 | **重啟 VSCode**，確認畫面顯示「container bash」 |  |\n",
      "| 4 | **在 VSCode 指令面板（Ctrl+Shift+P）輸入** `Reopen in Container` |  |\n",
      "\n",
      "> 這一步會把整個專案打包進一個 Docker 容器，並在容器內開啟 VSCode。  \n",
      "> 若你已經在容器內，直接跳到第 5 步。\n",
      "\n",
      "---\n",
      "\n",
      "## 2️⃣ 設定 VSCode 的 `launch.json`\n",
      "\n",
      "1. 在 VSCode 左側「Run」面板（或 `Ctrl+Shift+D`）點擊 **Create a launch.json file**。  \n",
      "2. 選擇 **Go**，並貼上以下設定：\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"version\": \"0.2.0\",\n",
      "    \"configurations\": [\n",
      "        {\n",
      "            \"name\": \"Launch Package\",\n",
      "            \"type\": \"go\",\n",
      "            \"request\": \"launch\",\n",
      "            \"mode\": \"auto\",\n",
      "            \"program\": \"${fileDirname}\"\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "```\n",
      "\n",
      "> - `program` 指向你想要執行的 Go package（通常是你目前開啟的檔案所在目錄）。  \n",
      "> - `mode: \"auto\"` 會自動決定是 `debug` 還是 `test`。\n",
      "\n",
      "---\n",
      "\n",
      "## 3️⃣ 進行 Debug 執行\n",
      "\n",
      "1. 在 VSCode 左側「Run」面板，選擇 **Launch Package**。  \n",
      "2. 點擊綠色 ▶️ 按鈕，VSCode 會在容器內啟動 `go run`，並進入 debug 模式。  \n",
      "3. 你可以在程式碼中設置斷點、觀察變數、步進執行等。\n",
      "\n",
      "---\n",
      "\n",
      "## 4️⃣ 常見錯誤與解決方案\n",
      "\n",
      "| 錯誤訊息 | 可能原因 | 解決方法 |\n",
      "|----------|----------|----------|\n",
      "| `error: Invalid userlane-proxy-path: userland-proxy is enabled, but userland-proxy-path is not set` | Docker daemon 的 `userland-proxy` 設定不正確 | 在容器內修改 `/etc/docker/daemon.json`，加入 `userland-proxy: false`，或確保 `userland-proxy-path` 已設定。 |\n",
      "| `Running modprobe bridge br_netfilter failed with message: \" error=\"exec: \\\"modprobe\\\": executable file not found in $PATH` | `modprobe` 未安裝或不在 `$PATH` | 在容器內執行：<br>`apt remove iptables`<br>`apt install iptables`（重新安裝 iptables 會帶入 modprobe） |\n",
      "| `iptables` 相關錯誤 | Docker 需要 iptables 但容器缺少 | 同上：重新安裝 iptables。 |\n",
      "\n",
      "> **備註**：如果你在容器內執行 `docker run hello-world` 仍然失敗，請確認容器內已安裝 `iptables` 並且 `dockerd` 正在執行。\n",
      "\n",
      "---\n",
      "\n",
      "## 5️⃣ 進階：在 Dev Container 內跑 Moby\n",
      "\n",
      "如果你想在 Dev Container 內直接跑 Moby（而不是單純 debug Go 程式），可以參考以下流程（來自 `set-up-dev-env.md`）：\n",
      "\n",
      "1. **在宿主機（VM）安裝 Docker**。  \n",
      "2. **在宿主機 clone 專案並建立容器**：\n",
      "\n",
      "   ```bash\n",
      "   git clone https://github.com/moby/moby\n",
      "   cd moby\n",
      "   git checkout -b dry-run-test\n",
      "   make BIND_DIR=. shell\n",
      "   ```\n",
      "\n",
      "3. **在容器內重新安裝 iptables，並啟動 dockerd**：\n",
      "\n",
      "   ```bash\n",
      "   apt remove iptables\n",
      "   apt install iptables\n",
      "   hack/make.sh binary\n",
      "   make install\n",
      "   dockerd -D\n",
      "   ```\n",
      "\n",
      "4. **在宿主機測試**：\n",
      "\n",
      "   ```bash\n",
      "   docker run hello-world\n",
      "   # Hello from Docker!\n",
      "   ```\n",
      "\n",
      "> 這樣你就能在 Dev Container 內完整跑 Moby，並可在 VSCode 內進行 debug。\n",
      "\n",
      "---\n",
      "\n",
      "## 6️⃣ 小結\n",
      "\n",
      "1. 安裝 VSCode + Dev Containers。  \n",
      "2. 在容器內打開 VSCode，設定 `launch.json`。  \n",
      "3. 執行 `Launch Package` 進入 debug。  \n",
      "4. 若遇到 `userland-proxy` 或 `modprobe` 的錯誤，重新安裝 iptables。  \n",
      "5. 如需完整跑 Moby，參照 Dev Container 的步驟。\n",
      "\n",
      "只要按上述順序操作，應該就能順利在 VSCode 內以 debug mode 執行 Moby Project。祝開發順利 🚀\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You >  what is moby project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================route_question\n",
      "content='DevEnvAgent' additional_kwargs={} response_metadata={'model': 'gpt-oss:20b', 'created_at': '2025-09-14T13:51:52.062943721Z', 'done': True, 'done_reason': 'stop', 'total_duration': 5716286111, 'load_duration': 2411111516, 'prompt_eval_count': 246, 'prompt_eval_duration': 256729838, 'eval_count': 192, 'eval_duration': 3012339197, 'model_name': 'gpt-oss:20b'} id='run--6f85672c-a19e-4676-afbb-fc7c6458eed7-0' usage_metadata={'input_tokens': 246, 'output_tokens': 192, 'total_tokens': 438}\n",
      "=================route_question\n",
      "DevEnvAgent\n",
      "[Router → DevEnvAgent]\n",
      "**Moby Project**  \n",
      "The Moby Project is the open‑source foundation that Docker is built on.  \n",
      "Its source code and contribution guidelines are hosted on GitHub:\n",
      "\n",
      "- Repository: <https://github.com/moby/moby>  \n",
      "- Contribution docs: <https://github.com/moby/moby/tree/master/docs/contributing>  \n",
      "\n",
      "The project contains the core components that make up Docker, and developers can contribute to it following the coding style and guidelines listed in the repository.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You >  怎麼安裝moby\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================route_question\n",
      "content='DevEnvAgent' additional_kwargs={} response_metadata={'model': 'gpt-oss:20b', 'created_at': '2025-09-14T13:53:05.50495072Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1646232821, 'load_duration': 84316729, 'prompt_eval_count': 247, 'prompt_eval_duration': 123941589, 'eval_count': 92, 'eval_duration': 1435741526, 'model_name': 'gpt-oss:20b'} id='run--a4b23224-cced-453f-a224-ad63ceb40eeb-0' usage_metadata={'input_tokens': 247, 'output_tokens': 92, 'total_tokens': 339}\n",
      "=================route_question\n",
      "DevEnvAgent\n",
      "[Router → DevEnvAgent]\n",
      "## 如何安裝 Moby\n",
      "\n",
      "以下列出最常用的安裝方式，僅依照你提供的文件內容。\n",
      "\n",
      "---\n",
      "\n",
      "### 1. 直接在宿主機（VM）上安裝\n",
      "\n",
      "1. **安裝 Docker**  \n",
      "   在宿主機（例如 Rocky Linux 8）上先安裝 Docker，這是執行 Moby 的前置條件。  \n",
      "   ```bash\n",
      "   # 以 Rocky Linux 8 為例\n",
      "   sudo dnf install -y docker\n",
      "   sudo systemctl enable --now docker\n",
      "   ```\n",
      "\n",
      "2. **克隆 Moby 專案**  \n",
      "   ```bash\n",
      "   git clone https://github.com/moby/moby\n",
      "   cd moby\n",
      "   ```\n",
      "\n",
      "3. **切換到你想要的分支或建立新分支**  \n",
      "   ```bash\n",
      "   git checkout -b dry-run-test   # 例如建立一個測試分支\n",
      "   ```\n",
      "\n",
      "4. **編譯並安裝 Moby**  \n",
      "   ```bash\n",
      "   # 先編譯二進位檔\n",
      "   hack/make.sh binary\n",
      "   # 安裝到系統\n",
      "   make install\n",
      "   ```\n",
      "\n",
      "5. **啟動 Docker Daemon（dockerd）**  \n",
      "   ```bash\n",
      "   dockerd -D\n",
      "   ```\n",
      "\n",
      "6. **測試**  \n",
      "   ```bash\n",
      "   docker run hello-world\n",
      "   # 你應該會看到 \"Hello from Docker!\" 的訊息\n",
      "   ```\n",
      "\n",
      "---\n",
      "\n",
      "### 2. 在 VSCode Dev Container 內部安裝\n",
      "\n",
      "1. **安裝 VSCode 插件**  \n",
      "   - Dev Containers\n",
      "\n",
      "2. **重啟 VSCode**  \n",
      "   - 會顯示 container bash\n",
      "\n",
      "3. **重新開啟容器**  \n",
      "   - VSCode 指令：`Reopen in Container`\n",
      "\n",
      "4. **在容器內部執行安裝步驟**  \n",
      "   ```bash\n",
      "   git clone https://github.com/moby/moby\n",
      "   cd moby\n",
      "   git checkout -b dry-run-test\n",
      "   make BIND_DIR=. shell   # 進入 shell\n",
      "   # 之後在 shell 裡執行\n",
      "   hack/make.sh binary\n",
      "   make install\n",
      "   dockerd -D\n",
      "   ```\n",
      "\n",
      "5. **測試**  \n",
      "   ```bash\n",
      "   docker run hello-world\n",
      "   ```\n",
      "\n",
      "---\n",
      "\n",
      "### 3. 需要重新安裝 iptables（若遇到相關錯誤）\n",
      "\n",
      "```bash\n",
      "sudo apt remove iptables\n",
      "sudo apt install iptables\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### 4. 進階：在 VSCode 內部以 Debug 模式執行 Moby\n",
      "\n",
      "1. **設定 `launch.json`**  \n",
      "   ```json\n",
      "   {\n",
      "       \"version\": \"0.2.0\",\n",
      "       \"configurations\": [\n",
      "           {\n",
      "               \"name\": \"Launch Package\",\n",
      "               \"type\": \"go\",\n",
      "               \"request\": \"launch\",\n",
      "               \"mode\": \"auto\",\n",
      "               \"program\": \"${fileDirname}\"\n",
      "           }\n",
      "       ]\n",
      "   }\n",
      "   ```\n",
      "\n",
      "2. **執行 `docker.go`**  \n",
      "   - 在 VSCode 內部以 Debug 模式執行 `docker.go`。\n",
      "\n",
      "---\n",
      "\n",
      "> **備註**  \n",
      "> - 若在容器內部執行 `dockerd` 時遇到 `error: Invalid userlane-proxy-path` 或 `Running modprobe bridge br_netfilter failed`，請參考文件中提供的 `daemon.json` 範例或重新安裝 iptables。  \n",
      "> - 若要在 Rocky Linux 8 VM 上使用 Docker Swarm，請先停用 `firewalld`：  \n",
      ">   ```bash\n",
      ">   systemctl stop firewalld\n",
      ">   systemctl disable firewalld\n",
      ">   ```\n",
      "\n",
      "這些步驟即可完成 Moby 的安裝與基本測試。祝你開發順利！\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "from typing import TypedDict, Dict, Any\n",
    "from pathlib import Path\n",
    "\n",
    "# LangChain - Ollama\n",
    "try:\n",
    "    from langchain_ollama import ChatOllama\n",
    "except ImportError:\n",
    "    from langchain_community.chat_models import ChatOllama  # 舊版用這個\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "# LangGraph\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# 載入本地 Prompt\n",
    "# ------------------------\n",
    "PROMPT_FILES = {\n",
    "    \"DevEnvAgent\": Path(\"DevEnvAgent.txt\"),\n",
    "    \"IssueAgent\": Path(\"IssueAgent.txt\"),\n",
    "    \"CodeTraceAgent\": Path(\"CodeTraceAgent.txt\"),\n",
    "    \"TestingAgent\": Path(\"TestingAgent.txt\"),\n",
    "}\n",
    "\n",
    "PROMPTS: Dict[str, str] = {k: v.read_text(encoding=\"utf-8\") for k, v in PROMPT_FILES.items()}\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# LLM (Ollama)\n",
    "# ------------------------\n",
    "llm = ChatOllama(\n",
    "    base_url=\"http://10.1.1.59:11434\",   # 確保 ollama 服務在本機 11434\n",
    "    model=\"gpt-oss:20b\",   # 換成你本地可用的模型\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Agent 工廠\n",
    "# ------------------------\n",
    "def make_agent(system_prompt: str):\n",
    "    # 用 f-string 把文件內容塞進 system，不讓 LangChain 解析裡面的 {}\n",
    "    template = \"\"\"{system_prompt}\n",
    "\n",
    "使用者的問題如下：\n",
    "{user_input}\n",
    "\"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    chain = prompt.partial(system_prompt=system_prompt.strip()) | llm | StrOutputParser()\n",
    "    return chain\n",
    "\n",
    "AGENTS = {name: make_agent(prompt) for name, prompt in PROMPTS.items()}\n",
    "\n",
    "\n",
    "# RouterAgent Prompt\n",
    "ROUTER_PROMPT = \"\"\"\n",
    "你是一個問題分流 Agent。  \n",
    "你的任務是根據使用者的輸入，判斷應該交給哪個專家 Agent 處理。  \n",
    "請只輸出對應的 Agent 名稱，不要輸出多餘文字。  \n",
    "\n",
    "可選的 Agent 類別有：\n",
    "- DevEnvAgent → 開發環境、VSCode、Dev Container、iptables、dockerd、debug mode\n",
    "- IssueAgent → GitHub Issue、Swarm 問題、錯誤訊息診斷\n",
    "- CodeTraceAgent → Code Trace、Container 啟動邏輯、containerd、gRPC\n",
    "- TestingAgent → 測試、unit test、integration test、TESTFLAGS、bridge_test.go\n",
    "\n",
    "輸出格式：\n",
    "<AgentName>\n",
    "\"\"\"\n",
    "\n",
    "# 建立 Prompt 模板\n",
    "router_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", ROUTER_PROMPT),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "router_chain = router_prompt | llm\n",
    "\n",
    "def route_question(user_question: str) -> str:\n",
    "    \"\"\"呼叫 LLM 來判斷要分派的 Agent\"\"\"\n",
    "    response = router_chain.invoke({\"question\": user_question})\n",
    "    output = response.content\n",
    "    print(\"=================route_question\")\n",
    "    print(response)\n",
    "    print(\"=================route_question\")\n",
    "    return output\n",
    " \n",
    "\n",
    "# ------------------------\n",
    "# LangGraph 狀態定義\n",
    "# ------------------------\n",
    "class GraphState(TypedDict):\n",
    "    user_input: str\n",
    "    routed: str | None\n",
    "    response: str\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Graph 節點\n",
    "# ------------------------\n",
    "def router_node(state: GraphState) -> GraphState:\n",
    "    agent_name = route_question(state[\"user_input\"])\n",
    "    print(agent_name)\n",
    "    return {**state, \"routed\": agent_name}\n",
    "\n",
    "def agent_node(state: GraphState) -> GraphState:\n",
    "    agent_name = state[\"routed\"]\n",
    "    chain = AGENTS[agent_name]\n",
    "    output = chain.invoke({\"user_input\": state[\"user_input\"]})\n",
    "    return {**state, \"response\": output}\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# 建立 Graph\n",
    "# ------------------------\n",
    "graph = StateGraph(GraphState)\n",
    "graph.add_node(\"router\", router_node)\n",
    "graph.add_node(\"agent\", agent_node)\n",
    "\n",
    "graph.set_entry_point(\"router\")\n",
    "graph.add_edge(\"router\", \"agent\")\n",
    "graph.add_edge(\"agent\", END)\n",
    "\n",
    "app = graph.compile()\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# 測試用函數\n",
    "# ------------------------\n",
    "def run_once(user_text: str) -> Dict[str, Any]:\n",
    "    state = {\"user_input\": user_text, \"routed\": None, \"response\": \"\"}\n",
    "    result = app.invoke(state)\n",
    "    return {\n",
    "        \"routed_agent\": result[\"routed\"],\n",
    "        \"response\": result[\"response\"]\n",
    "    }\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# CLI 互動\n",
    "# ------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== RouterAgent + 4 Agents (Ollama) ===\")\n",
    "    while True:\n",
    "        q = input(\"\\nYou > \").strip()\n",
    "        if q.lower() in {\"exit\", \"quit\"}:\n",
    "            break\n",
    "        out = run_once(q)\n",
    "        print(f\"[Router → {out['routed_agent']}]\")\n",
    "        print(out[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad13824e-7e09-40f3-bac3-b324b9ad254d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
