{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef5c0d57-002b-4adc-a6e6-d606277fc89d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2739/424442926.py:4: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  ollama_agent = ChatOllama(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------research node-----------------\n",
      "## Ollama â€“ A Quickâ€‘start Guide to the â€œLocalâ€‘LLMâ€ Ecosystem\n",
      "\n",
      "**What is Ollama?**  \n",
      "Ollama is a lightweight, openâ€‘source framework that lets you download, run, and manage largeâ€‘languageâ€‘models (LLMs) on your own hardwareâ€”no cloud connection required. Think of it as a â€œModel Hub + Runtimeâ€ for LLMs that lives on your laptop, desktop, or even a small edge device. It was launched in 2023 by **Ollama, Inc.** (the same folks behind the popular *Ollama* chat app that runs locally) and has quickly become a goâ€‘to solution for developers, researchers, and hobbyists who want privacyâ€‘first, lowâ€‘latency AI without paying perâ€‘token fees.\n",
      "\n",
      "---\n",
      "\n",
      "### 1. The Big Picture\n",
      "\n",
      "| Aspect | What Ollama Provides | Why It Matters |\n",
      "|--------|---------------------|----------------|\n",
      "| **Local inference** | Run any LLM on your GPU/CPU without a server | Keeps data onâ€‘device, eliminates latency & cost |\n",
      "| **Model hub** | Preâ€‘built, readyâ€‘toâ€‘use weights (Llama 3, Gemma, Mistral, etc.) | No manual downloads or complicated setup |\n",
      "| **Unified API** | Simple CLI & HTTP REST interface | Quick prototyping & productionâ€‘grade integration |\n",
      "| **Crossâ€‘platform** | Windows, macOS, Linux, Raspberryâ€‘Pi, Android | Works on almost any machine |\n",
      "| **Open source** | All code (including the CLI, server, and web UI) is on GitHub | Inspect, modify, or contribute |\n",
      "\n",
      "---\n",
      "\n",
      "### 2. Core Components\n",
      "\n",
      "| Component | Role | Tech Stack |\n",
      "|-----------|------|------------|\n",
      "| **`ollama` CLI** | Pull models, run inference, manage local cache | Go (frontend), Rust (backend) |\n",
      "| **`ollama` daemon** | HTTP server exposing `/api/generate` and `/api/embeddings` | Go |\n",
      "| **Web UI** | Quickâ€‘start chat interface (no code) | Vanilla JS/HTML/CSS |\n",
      "| **Python wrapper** | `ollama-python` library for programmatic access | Python 3.9+ |\n",
      "| **Community libraries** | e.g., `ollama-node`, `ollama-go` | Various languages |\n",
      "\n",
      "---\n",
      "\n",
      "### 3. How It Works Under the Hood\n",
      "\n",
      "1. **Model Storage**  \n",
      "   - Models are stored in a local folder (`~/.ollama/models/`).  \n",
      "   - Ollama uses the **ONNX**/`ggml` format for compact binary weights, enabling fast CPU inference when GPUs arenâ€™t available.\n",
      "\n",
      "2. **Inference Engine**  \n",
      "   - GPU acceleration via CUDA (NVIDIA), Metal (Apple), or Vulkan/ROCm.  \n",
      "   - CPU fallback uses a highly optimized Rust engine that can run on singleâ€‘core machines.\n",
      "\n",
      "3. **Prompting**  \n",
      "   - Supports the same prompt templates used by OpenAI, making it trivial to swap between Ollama and the OpenAI API.  \n",
      "   - `--prompt` flag lets you pass text or a JSON prompt for advanced token control.\n",
      "\n",
      "4. **API**  \n",
      "   - `POST /api/generate` â€“ stream or block text generation.  \n",
      "   - `POST /api/embeddings` â€“ vector embeddings for semantic search.  \n",
      "   - `GET /api/models` â€“ list available models locally.  \n",
      "\n",
      "---\n",
      "\n",
      "### 4. Using Ollama â€“ A Minimal Example\n",
      "\n",
      "```bash\n",
      "# 1. Install (macOS example)\n",
      "brew install ollama\n",
      "\n",
      "# 2. Pull a model\n",
      "ollama pull llama3.1\n",
      "\n",
      "# 3. Run it locally\n",
      "ollama run llama3.1 \"Explain Ollama in 3 sentences.\"\n",
      "\n",
      "# 4. Using the HTTP API (curl)\n",
      "curl http://localhost:11434/api/generate \\\n",
      "  -H \"Content-Type: application/json\" \\\n",
      "  -d '{\"model\":\"llama3.1\",\"prompt\":\"What is Ollama?\",\"stream\":true}'\n",
      "```\n",
      "\n",
      "For developers, the Python library makes it even easier:\n",
      "\n",
      "```python\n",
      "import ollama\n",
      "\n",
      "# List local models\n",
      "print(ollama.list())\n",
      "\n",
      "# Generate text\n",
      "response = ollama.generate(\"llama3.1\", \"Describe the architecture of Ollama.\")\n",
      "print(response[\"response\"])\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### 5. Supported Models (as of 2025â€‘09)\n",
      "\n",
      "| Model | Origin | Size | Notes |\n",
      "|-------|--------|------|-------|\n",
      "| **Llama 3.1** | Meta | 8Bâ€“70B | Official weights, 8â€‘bit/16â€‘bit support |\n",
      "| **Gemma** | Google | 2Bâ€“8B | Lightweight, good for mobile |\n",
      "| **Mistral** | Mistral AI | 7B | Fast inference, openâ€‘source weights |\n",
      "| **Phiâ€‘2** | Microsoft | 2B | Very small, runs on even lowâ€‘end GPUs |\n",
      "| **Mixtralâ€‘8x7B** | MosaicML | 8Ã—7B | Multiâ€‘head mixtureâ€‘ofâ€‘experts |\n",
      "| **Code Llama** | Meta | 7B/13B | Codeâ€‘specialized LLM |\n",
      "| **OpenAI compatible** | Custom | 3B+ | Some custom models mimic OpenAI APIs |\n",
      "\n",
      "> **Tip**: If you need a model that isnâ€™t in the hub, you can upload your own ONNX or GGML file to `~/.ollama/models/`.\n",
      "\n",
      "---\n",
      "\n",
      "### 6. Use Cases\n",
      "\n",
      "| Category | Why Local Matters |\n",
      "|----------|-------------------|\n",
      "| **Privacyâ€‘sensitive apps** | All data stays on the device (e.g., personal noteâ€‘taking, medical records). |\n",
      "| **Offline mode** | Work in areas without internet or on lowâ€‘bandwidth networks. |\n",
      "| **Costâ€‘efficiency** | No perâ€‘token fees after initial download. |\n",
      "| **Rapid prototyping** | Spin up a model instantly, iterate locally, then deploy. |\n",
      "| **Research** | Full control over training data, model modifications, and debugging. |\n",
      "| **Edge devices** | Run LLM inference on Raspberryâ€‘Pi, Jetson, or embedded systems. |\n",
      "\n",
      "---\n",
      "\n",
      "### 7. Community & Ecosystem\n",
      "\n",
      "| Resource | What It Offers |\n",
      "|----------|----------------|\n",
      "| **GitHub Repo** (`https://github.com/ollama/ollama`) | Source, issues, PRs. |\n",
      "| **Discord / Slack** | Community support, feature requests. |\n",
      "| **StackOverflow** | Tag `ollama` for Q&A. |\n",
      "| **Reddit** (`r/ollama`) | Announcements, tutorials. |\n",
      "| **Blogs & Tutorials** | e.g., *Ollamaâ€™s official blog*, *Hugging Faceâ€™s â€œRun LLM locallyâ€* guide. |\n",
      "\n",
      "> **Contribution Note**: Ollama is actively maintained, with frequent releases. If you want to add a new model format or improve GPU support, the repo welcomes contributions.\n",
      "\n",
      "---\n",
      "\n",
      "### 8. Comparisons to Other LLM Platforms\n",
      "\n",
      "| Feature | Ollama | OpenAI API | LlamaIndex | Hugging Face Inference API |\n",
      "|---------|--------|------------|------------|----------------------------|\n",
      "| **Privacy** | Local only | Cloud | Depends on deployment | Cloud |\n",
      "| **Cost** | Zero after download | Tokenâ€‘based | Depends on host | Tokenâ€‘based |\n",
      "| **Latency** | Milliseconds (GPU) | 200â€‘500ms | Varies | 200â€‘500ms |\n",
      "| **Model Variety** | 30+ curated models | Few official models | Any LLM with LLMIndex | Any Hugging Face model |\n",
      "| **Ease of Setup** | 1â€‘click install | API key | Code + hosting | API key |\n",
      "| **Extensibility** | CLI + API + SDK | SDKs | SDKs | SDKs |\n",
      "| **Edge** | Yes | No | Yes (if selfâ€‘hosted) | No |\n",
      "\n",
      "---\n",
      "\n",
      "### 9. Potential Challenges & Mitigations\n",
      "\n",
      "| Challenge | How Ollama Addresses It |\n",
      "|-----------|------------------------|\n",
      "| **Large model size** | Uses compressed GGML/ONNX formats; supports quantization. |\n",
      "| **GPU requirements** | Works on CPU; offers 8â€‘bit quantization for lowâ€‘end GPUs. |\n",
      "| **Compatibility with new models** | Autoâ€‘detects model metadata; you can create custom `manifest.json`. |\n",
      "| **Security** | No data leaves the device; still need to guard local storage. |\n",
      "| **License compliance** | Ollama doesnâ€™t host proprietary weights; you must comply with each modelâ€™s license. |\n",
      "\n",
      "---\n",
      "\n",
      "### 10. Future Directions (What to Watch)\n",
      "\n",
      "1. **Model Hub Expansion** â€“ Adding more fineâ€‘tuned, specialized models (vision, multimodal).  \n",
      "2. **Containerized Deployment** â€“ Docker images for easy server setup.  \n",
      "3. **Autoâ€‘quantization tooling** â€“ Onâ€‘theâ€‘fly 4â€‘bit/8â€‘bit conversion for maximal efficiency.  \n",
      "4. **Federated Learning** â€“ Collaborative training while keeping data local.  \n",
      "5. **Communityâ€‘driven UI** â€“ More UI frameworks (React, Flutter) to embed chat widgets.  \n",
      "\n",
      "---\n",
      "\n",
      "## Quick Checklist for Getting Started\n",
      "\n",
      "1. **Install**: `brew install ollama` / `curl -fsSL https://ollama.com/install.sh | sh`  \n",
      "2. **Pull a Model**: `ollama pull llama3.1`  \n",
      "3. **Run**: `ollama run llama3.1 \"Hello world!\"`  \n",
      "4. **Use the API**: Point your app to `http://localhost:11434/api/generate`  \n",
      "5. **Optional**: Add `ollama` as a Python dependency and start coding!\n",
      "\n",
      "---\n",
      "\n",
      "### Final Thought\n",
      "\n",
      "Ollama redefines how we interact with large language models by **removing the middleman**. If youâ€™re building a privacyâ€‘first product, need lowâ€‘latency AI, or simply want to tinker with stateâ€‘ofâ€‘theâ€‘art models without recurring costs, Ollama gives you a **dropâ€‘in, openâ€‘source, localâ€‘first** solution that scales from a laptop to a small edge server.  \n",
      "\n",
      "Happy codingâ€”your local AI assistant is just a CLI command away!\n",
      "-----------summary node-----------------\n",
      "**Ollama é€Ÿæˆç²¾è¯ä¸‰å¤§é‡é»**  \n",
      "\n",
      "1. **å®Œå…¨æœ¬åœ°åŒ–ã€é–‹æºä¸”è¼•é‡åŒ–**  \n",
      "   - åªéœ€ä¸‹è¼‰æ¨¡å‹å³å¯åœ¨è‡ªå·±çš„ GPU / CPU ä¸Šç›´æ¥æ¨ç†ï¼Œç„¡éœ€é›²ç«¯é€£ç·šæˆ–ä»˜è²»ä½¿ç”¨ã€‚  \n",
      "   - é€é `ollama` CLIã€HTTP API ä»¥åŠ Python å¥—ä»¶ï¼Œæä¾›çµ±ä¸€ä¸”ç°¡æ½”çš„æ“ä½œä»‹é¢ã€‚  \n",
      "\n",
      "2. **æ¨¡å‹é›†å¸‚ + å³æ™‚æ¨ç†å¼•æ“**  \n",
      "   - å…§å»ºå¤šé” 30+ é è¨“ç·´æ¨¡å‹ï¼ˆLlamaâ€¯3ã€Gemmaã€Mistralã€Phiâ€‘2 ç­‰ï¼‰ï¼Œæ”¯æ´ ONNX/ggml æ ¼å¼ï¼Œä¾¿æ–¼å¿«é€Ÿå®‰è£ã€‚  \n",
      "   - GPU åŠ é€Ÿï¼ˆCUDAã€Metalã€Vulkan/ROCmï¼‰èˆ‡ CPU æœ€ä½³åŒ–å¼•æ“ä¸¦è¡Œï¼Œå¯åœ¨æ¡Œé¢ã€ç­†é›»ã€Raspberryâ€‘Piã€Android ç­‰å¤šå¹³å°é‹è¡Œã€‚  \n",
      "\n",
      "3. **éš±ç§ã€ä½å»¶é²èˆ‡æˆæœ¬å„ªå‹¢**  \n",
      "   - æ‰€æœ‰è³‡æ–™çš†ç•™åœ¨æœ¬æ©Ÿï¼Œå®Œå…¨ä¸éœ€ä¸Šå‚³é›²ç«¯ï¼Œé©åˆéš±ç§æ•æ„Ÿæˆ–é›¢ç·šä½œæ¥­ã€‚  \n",
      "   - åˆå§‹ä¸‹è¼‰å¾Œä¸å†ç”¢ç”Ÿ token è²»ç”¨ï¼Œä¸”æ¨ç†å»¶é²åƒ…æ•¸æ¯«ç§’ï¼ˆGPUï¼‰æˆ–ç§’ç´šï¼ˆCPUï¼‰ã€‚  \n",
      "   - é©ç”¨æ–¼å¿«é€ŸåŸå‹é–‹ç™¼ã€ç ”ç©¶ã€é‚Šç·£è£ç½®éƒ¨ç½²ä»¥åŠæˆæœ¬æ•æ„Ÿçš„ç”¢å“ã€‚\n",
      "-----------result-----------------\n",
      "**Ollama é€Ÿæˆç²¾è¯ä¸‰å¤§é‡é»**  \n",
      "\n",
      "1. **å®Œå…¨æœ¬åœ°åŒ–ã€é–‹æºä¸”è¼•é‡åŒ–**  \n",
      "   - åªéœ€ä¸‹è¼‰æ¨¡å‹å³å¯åœ¨è‡ªå·±çš„ GPU / CPU ä¸Šç›´æ¥æ¨ç†ï¼Œç„¡éœ€é›²ç«¯é€£ç·šæˆ–ä»˜è²»ä½¿ç”¨ã€‚  \n",
      "   - é€é `ollama` CLIã€HTTP API ä»¥åŠ Python å¥—ä»¶ï¼Œæä¾›çµ±ä¸€ä¸”ç°¡æ½”çš„æ“ä½œä»‹é¢ã€‚  \n",
      "\n",
      "2. **æ¨¡å‹é›†å¸‚ + å³æ™‚æ¨ç†å¼•æ“**  \n",
      "   - å…§å»ºå¤šé” 30+ é è¨“ç·´æ¨¡å‹ï¼ˆLlamaâ€¯3ã€Gemmaã€Mistralã€Phiâ€‘2 ç­‰ï¼‰ï¼Œæ”¯æ´ ONNX/ggml æ ¼å¼ï¼Œä¾¿æ–¼å¿«é€Ÿå®‰è£ã€‚  \n",
      "   - GPU åŠ é€Ÿï¼ˆCUDAã€Metalã€Vulkan/ROCmï¼‰èˆ‡ CPU æœ€ä½³åŒ–å¼•æ“ä¸¦è¡Œï¼Œå¯åœ¨æ¡Œé¢ã€ç­†é›»ã€Raspberryâ€‘Piã€Android ç­‰å¤šå¹³å°é‹è¡Œã€‚  \n",
      "\n",
      "3. **éš±ç§ã€ä½å»¶é²èˆ‡æˆæœ¬å„ªå‹¢**  \n",
      "   - æ‰€æœ‰è³‡æ–™çš†ç•™åœ¨æœ¬æ©Ÿï¼Œå®Œå…¨ä¸éœ€ä¸Šå‚³é›²ç«¯ï¼Œé©åˆéš±ç§æ•æ„Ÿæˆ–é›¢ç·šä½œæ¥­ã€‚  \n",
      "   - åˆå§‹ä¸‹è¼‰å¾Œä¸å†ç”¢ç”Ÿ token è²»ç”¨ï¼Œä¸”æ¨ç†å»¶é²åƒ…æ•¸æ¯«ç§’ï¼ˆGPUï¼‰æˆ–ç§’ç´šï¼ˆCPUï¼‰ã€‚  \n",
      "   - é©ç”¨æ–¼å¿«é€ŸåŸå‹é–‹ç™¼ã€ç ”ç©¶ã€é‚Šç·£è£ç½®éƒ¨ç½²ä»¥åŠæˆæœ¬æ•æ„Ÿçš„ç”¢å“ã€‚\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from typing import TypedDict\n",
    "\n",
    "ollama_agent = ChatOllama(\n",
    "    model=\"gpt-oss:20b\",\n",
    "    base_url=\"http://10.1.1.59:11434\"\n",
    ")\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    query: str\n",
    "    research: str\n",
    "    summary: str\n",
    "# Agent A: Research Agent\n",
    "def research_node(state: State):\n",
    "    print(\"-----------research node-----------------\")\n",
    "    q = state[\"query\"]\n",
    "    res = ollama_agent.invoke(f\"Explain information about the following topic: {q}\")\n",
    "    state[\"research\"] = res.content\n",
    "    print(res.content)\n",
    "    return state\n",
    "\n",
    "# Agent B: Summarizer Agent\n",
    "def summary_node(state: State):\n",
    "    print(\"-----------summary node-----------------\")\n",
    "    text = state[\"research\"]\n",
    "    res = ollama_agent.invoke(f\"Use Traditional Chinese. Summarize the following content into three key points: {text}\")\n",
    "    state[\"summary\"] = res.content\n",
    "    print(res.content)\n",
    "    return state\n",
    "\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "graph = StateGraph(State)\n",
    "\n",
    "graph.add_node(\"ResearchAgent\", research_node)\n",
    "graph.add_node(\"SummarizerAgent\", summary_node)\n",
    "\n",
    "graph.set_entry_point(\"ResearchAgent\")\n",
    "graph.add_edge(\"ResearchAgent\", \"SummarizerAgent\")\n",
    "graph.add_edge(\"SummarizerAgent\", END)\n",
    "\n",
    "app = graph.compile()\n",
    "\n",
    "resp = app.invoke({\"query\": \"Ollama\"})\n",
    "\n",
    "print(\"-----------result-----------------\")\n",
    "print(resp[\"summary\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e7c9379-cbcb-4de6-a005-a59fc9da10c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Go è£¡çš„ := æ˜¯ä»€éº¼æ„æ€ï¼Ÿ\n",
      "â†’ åˆ†é…çµ¦: {\n",
      "  \"agent\": \"SyntaxCoachAgent\"\n",
      "}\n",
      "\n",
      "Q: å¹«æˆ‘å¯«ä¸€å€‹ Go REST API ç¯„ä¾‹\n",
      "â†’ åˆ†é…çµ¦: {\n",
      "  \"agent\": \"PracticalExampleAgent\"\n",
      "}\n",
      "\n",
      "Q: æˆ‘å®‰è£ Go ä¹‹å¾ŒåŸ·è¡Œ go run å ±éŒ¯\n",
      "â†’ åˆ†é…çµ¦: {\n",
      "  \"agent\": \"TroubleshootingAgent\"\n",
      "}\n",
      "\n",
      "Q: è«‹å¹«æˆ‘è¦åŠƒä¸€ä»½å­¸ç¿’ Golang çš„çŸ¥è­˜æ¶æ§‹\n",
      "â†’ åˆ†é…çµ¦: {\n",
      "  \"agent\": \"KnowledgeTreeAgent\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from typing import TypedDict\n",
    "\n",
    "ollama_agent = ChatOllama(\n",
    "    model=\"gpt-oss:20b\",\n",
    "    base_url=\"http://10.1.1.59:11434\"\n",
    ")\n",
    "\n",
    "ROUTER_PROMPT = \"\"\"\n",
    "ä½ æ˜¯ä¸€å€‹å•é¡Œåˆ†æµ Agentã€‚  \n",
    "ä½ çš„ä»»å‹™æ˜¯æ ¹æ“šä½¿ç”¨è€…çš„è¼¸å…¥ï¼Œåˆ¤æ–·æ‡‰è©²ç”±å“ªå€‹å°ˆå®¶ Agent è™•ç†ã€‚  \n",
    "è«‹åªè¼¸å‡ºå°æ‡‰çš„ Agent åç¨±ï¼Œä¸è¦è¼¸å‡ºå¤šé¤˜æ–‡å­—ã€‚  \n",
    "\n",
    "å¯é¸çš„ Agent é¡åˆ¥æœ‰ï¼š  \n",
    "- TutorialAgent â†’ å®‰è£ã€ç’°å¢ƒè¨­å®šã€å…¥é–€æŒ‡å¼•  \n",
    "- SyntaxCoachAgent â†’ Go èªæ³•ã€è³‡æ–™å‹æ…‹ã€é—œéµå­—è§£é‡‹ï¼ˆä¾‹å¦‚ :=, defer, map, struct, interfaceï¼‰  \n",
    "- PracticalExampleAgent â†’ REST APIã€æ¸¬è©¦ã€ç¯„ä¾‹ç¨‹å¼ç¢¼ã€æ¨¡çµ„æ‡‰ç”¨  \n",
    "- ConcurrencyExpertAgent â†’ goroutinesã€channelsã€selectã€åŒæ­¥/éåŒæ­¥ç¨‹å¼è¨­è¨ˆ  \n",
    "- TroubleshootingAgent â†’ éŒ¯èª¤è¨Šæ¯è¨ºæ–·ã€ç·¨è­¯å•é¡Œã€æ¨¡çµ„ç®¡ç†éŒ¯èª¤  \n",
    "- KnowledgeTreeAgent â†’ ç³»çµ±åŒ–çŸ¥è­˜çµæ§‹ã€å­¸ç¿’è·¯å¾‘ã€çŸ¥è­˜æ¨¹  \n",
    "\n",
    "è¼¸å‡ºæ ¼å¼ï¼ˆJSONï¼‰ï¼š  \n",
    "{\n",
    "  \"agent\": \"<AgentName>\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def route_question(user_question: str) -> str:\n",
    "    \"\"\"RouterAgent: åˆ†æ´¾ä½¿ç”¨è€…çš„å•é¡Œåˆ°å°æ‡‰çš„ Agent\"\"\"\n",
    "    \n",
    "    prompt = ROUTER_PROMPT + f\"\\n\\nä½¿ç”¨è€…çš„è¼¸å…¥æ˜¯ï¼š{user_question}\"\n",
    "    # print(prompt)\n",
    "\n",
    "    response = ollama_agent.invoke(prompt)   # ç”¨ langchain_community ChatOllama\n",
    "    output = response.content.strip()\n",
    "\n",
    "    return output\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_questions = [\n",
    "        \"Go è£¡çš„ := æ˜¯ä»€éº¼æ„æ€ï¼Ÿ\",\n",
    "        \"å¹«æˆ‘å¯«ä¸€å€‹ Go REST API ç¯„ä¾‹\",\n",
    "        \"æˆ‘å®‰è£ Go ä¹‹å¾ŒåŸ·è¡Œ go run å ±éŒ¯\",\n",
    "        \"è«‹å¹«æˆ‘è¦åŠƒä¸€ä»½å­¸ç¿’ Golang çš„çŸ¥è­˜æ¶æ§‹\",\n",
    "    ]\n",
    "\n",
    "    for q in test_questions:\n",
    "        agent = route_question(q)\n",
    "        print(f\"Q: {q}\\nâ†’ åˆ†é…çµ¦: {agent}\\n\")\n",
    "# graph = StateGraph()\n",
    "\n",
    "# # ç¯€é»å®šç¾©\n",
    "# graph.add_node(\"RouterAgent\", router_agent)\n",
    "# graph.add_node(\"TutorialAgent\", tutorial_agent)\n",
    "# graph.add_node(\"SyntaxCoachAgent\", syntax_coach_agent)\n",
    "# graph.add_node(\"PracticalExampleAgent\", practical_example_agent)\n",
    "# graph.add_node(\"ConcurrencyExpertAgent\", concurrency_expert_agent)\n",
    "# graph.add_node(\"TroubleshootingAgent\", troubleshooting_agent)\n",
    "# graph.add_node(\"KnowledgeTreeAgent\", knowledge_tree_agent)\n",
    "# graph.add_node(\"AnswerCollector\", answer_collector)\n",
    "\n",
    "# # é‚Š (Router â†’ å°æ‡‰ Agent)\n",
    "# graph.add_edge(\"RouterAgent\", \"TutorialAgent\")\n",
    "# graph.add_edge(\"RouterAgent\", \"SyntaxCoachAgent\")\n",
    "# graph.add_edge(\"RouterAgent\", \"PracticalExampleAgent\")\n",
    "# graph.add_edge(\"RouterAgent\", \"ConcurrencyExpertAgent\")\n",
    "# graph.add_edge(\"RouterAgent\", \"TroubleshootingAgent\")\n",
    "# graph.add_edge(\"RouterAgent\", \"KnowledgeTreeAgent\")\n",
    "\n",
    "# # Agent â†’ AnswerCollector\n",
    "# for agent in [\n",
    "#     \"TutorialAgent\", \"SyntaxCoachAgent\", \"PracticalExampleAgent\",\n",
    "#     \"ConcurrencyExpertAgent\", \"TroubleshootingAgent\", \"KnowledgeTreeAgent\"\n",
    "# ]:\n",
    "#     graph.add_edge(agent, \"AnswerCollector\")\n",
    "\n",
    "# # AnswerCollector â†’ END\n",
    "# graph.add_edge(\"AnswerCollector\", END)\n",
    "\n",
    "# # å…¥å£\n",
    "# graph.set_entry_point(\"RouterAgent\")\n",
    "\n",
    "# # å»ºç«‹æ‡‰ç”¨\n",
    "# app = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3323e2bd-0877-4dc9-b596-4919e7289d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RouterAgent + 4 Agents (Ollama) ===\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You >  run vscode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================route_question\n",
      "content='DevEnvAgent' additional_kwargs={} response_metadata={'model': 'gpt-oss:20b', 'created_at': '2025-09-14T13:17:31.399820097Z', 'done': True, 'done_reason': 'stop', 'total_duration': 619140148, 'load_duration': 82524262, 'prompt_eval_count': 243, 'prompt_eval_duration': 29764787, 'eval_count': 33, 'eval_duration': 504725680, 'model_name': 'gpt-oss:20b'} id='run--9dfbca75-e637-4861-9823-d5a044618965-0' usage_metadata={'input_tokens': 243, 'output_tokens': 33, 'total_tokens': 276}\n",
      "=================route_question\n",
      "DevEnvAgent\n",
      "[Router â†’ DevEnvAgent]\n",
      "ä»¥ä¸‹æ˜¯ä½¿ç”¨ VSCode ä¾†å•Ÿå‹•ä¸¦é™¤éŒ¯ Moby å°ˆæ¡ˆçš„å®Œæ•´æµç¨‹ï¼ˆåŒ…å« Dev Container èˆ‡ Debug æ¨¡å¼ï¼‰ï¼š\n",
      "\n",
      "---\n",
      "\n",
      "## 1ï¸âƒ£ å…ˆæ±ºæ¢ä»¶\n",
      "\n",
      "| é …ç›® | éœ€æ±‚ | åƒè€ƒ |\n",
      "|------|------|------|\n",
      "| VSCode | ç‰ˆæœ¬ 1.70+ |  |\n",
      "| VSCode æ’ä»¶ | **Dev Containers** |  |\n",
      "| Docker | å®‰è£åœ¨å®¿ä¸»æ©Ÿï¼ˆVMï¼‰ä¸Š |  |\n",
      "| ç›®éŒ„ | ä½ å·²ç¶“ clone ä¸‹ä¾†çš„ `moby` å°ˆæ¡ˆ |  |\n",
      "\n",
      "> **æç¤º**ï¼šå¦‚æœä½ é‚„æ²’å®‰è£ Dockerï¼Œè«‹å…ˆåœ¨å®¿ä¸»æ©Ÿä¸Šå®‰è£ä¸¦ç¢ºèª `docker run hello-world` èƒ½æ­£å¸¸åŸ·è¡Œã€‚\n",
      "\n",
      "---\n",
      "\n",
      "## 2ï¸âƒ£ åœ¨ VSCode å…§éƒ¨å•Ÿå‹• Dev Container\n",
      "\n",
      "1. **æ‰“é–‹ VSCode**  \n",
      "   - é€²å…¥ä½  clone ä¸‹ä¾†çš„ `moby` å°ˆæ¡ˆè³‡æ–™å¤¾ã€‚\n",
      "\n",
      "2. **å®‰è£ Dev Containers æ’ä»¶**  \n",
      "   - åœ¨å·¦å´æ“´å……åŠŸèƒ½é¢æ¿æœå°‹ `Dev Containers`ï¼Œé»æ“Šå®‰è£ã€‚\n",
      "\n",
      "3. **é‡å•Ÿ VSCode**  \n",
      "   - å®‰è£å®Œæˆå¾Œï¼ŒVSCode æœƒæç¤ºä½ é‡å•Ÿï¼Œè«‹æŒ‰ä¸‹ **Reload**ã€‚\n",
      "\n",
      "4. **é–‹å•Ÿ Dev Container**  \n",
      "   - åœ¨ VSCode å‘½ä»¤é¢æ¿ï¼ˆ`Ctrl+Shift+P`ï¼‰è¼¸å…¥ `Dev Containers: Reopen in Container`ï¼Œé¸æ“‡å®ƒã€‚  \n",
      "   - VSCode æœƒè‡ªå‹•ä¸‹è¼‰ä¸¦å•Ÿå‹•å®¹å™¨ï¼Œä¸¦åœ¨å®¹å™¨å…§é¡¯ç¤ºä¸€å€‹ Bash ç»ˆç«¯ã€‚\n",
      "\n",
      "---\n",
      "\n",
      "## 3ï¸âƒ£ è¨­å®š Debug ç’°å¢ƒ (`launch.json`)\n",
      "\n",
      "1. åœ¨ VSCode å·¦å´çš„ **Run** é¢æ¿ï¼ˆæˆ–æŒ‰ `Ctrl+Shift+D`ï¼‰é»æ“Š **create a launch.json file**ã€‚  \n",
      "2. é¸æ“‡ **Go**ï¼Œç„¶å¾Œè²¼ä¸Šä»¥ä¸‹è¨­å®šï¼š\n",
      "\n",
      "   ```json\n",
      "   {\n",
      "       \"version\": \"0.2.0\",\n",
      "       \"configurations\": [\n",
      "           {\n",
      "               \"name\": \"Launch Package\",\n",
      "               \"type\": \"go\",\n",
      "               \"request\": \"launch\",\n",
      "               \"mode\": \"auto\",\n",
      "               \"program\": \"${fileDirname}\"\n",
      "           }\n",
      "       ]\n",
      "   }\n",
      "   ```\n",
      "\n",
      "3. é€™æ¨£å°±å¯ä»¥åœ¨å®¹å™¨å…§ç›´æ¥ä»¥ Go èªè¨€çš„æ–¹å¼é™¤éŒ¯ä»»ä½• Go æª”æ¡ˆã€‚\n",
      "\n",
      "---\n",
      "\n",
      "## 4ï¸âƒ£ å¸¸è¦‹éŒ¯èª¤èˆ‡è§£æ±ºæ–¹æ¡ˆ\n",
      "\n",
      "| éŒ¯èª¤ | å¯èƒ½åŸå›  | è§£æ±ºæ–¹æ³• |\n",
      "|------|----------|----------|\n",
      "| `error: Invalid userlane-proxy-path: userland-proxy is enabled, but userland-proxy-path is not set` | Docker daemon è¨­å®šç¼ºå°‘ `userland-proxy-path` | åœ¨ `/etc/docker/daemon.json` åŠ å…¥ `\"userland-proxy\": false` æˆ–è¨­å®šæ­£ç¢ºè·¯å¾‘ |\n",
      "| `Running modprobe bridge br_netfilter failed with message: \" error=\"exec: \\\"modprobe\\\": executable file not found in $PATH` | å®¹å™¨å…§ç¼ºå°‘ `modprobe` æˆ– `iptables` | é‡æ–°å®‰è£ iptablesï¼š`apt remove iptables && apt install iptables` |\n",
      "| `docker swarm init` å¤±æ•— | é˜²ç«ç‰†é˜»æ“‹ 2377 ç«¯å£ | åœ¨ Rocky Linux 8 VM ä¸Šåœç”¨ firewalldï¼š`systemctl stop firewalld && systemctl disable firewalld` |\n",
      "\n",
      "---\n",
      "\n",
      "## 5ï¸âƒ£ é€²éšï¼šåœ¨ VSCode å…§éƒ¨å•Ÿå‹• Docker Swarm\n",
      "\n",
      "1. **åœç”¨å®¿ä¸»æ©Ÿé˜²ç«ç‰†**ï¼ˆè‹¥ä½¿ç”¨ Rocky Linux 8ï¼‰  \n",
      "   ```bash\n",
      "   systemctl stop firewalld\n",
      "   systemctl disable firewalld\n",
      "   ```\n",
      "\n",
      "2. **åœ¨ VSCode Dev Container å…§éƒ¨åŸ·è¡Œ**  \n",
      "   ```bash\n",
      "   docker swarm init --advertise-addr <your-container-ip>\n",
      "   ```\n",
      "\n",
      "3. **ç¢ºèª Swarm ç«¯å£**  \n",
      "   ```bash\n",
      "   netstat -anp | grep :2377\n",
      "   ```\n",
      "\n",
      "4. **åœ¨å®¿ä¸»æ©Ÿï¼ˆæˆ–å¦ä¸€å° VMï¼‰åŠ å…¥ Swarm**  \n",
      "   ```bash\n",
      "   docker swarm join --token <token> <manager-ip>:2377\n",
      "   ```\n",
      "\n",
      "5. **åœ¨ VSCode å…§éƒ¨ç¢ºèª**  \n",
      "   - é€é `docker node ls` æˆ– `docker service ls` æŸ¥çœ‹ç¯€é»èˆ‡æœå‹™ã€‚\n",
      "\n",
      "---\n",
      "\n",
      "## 6ï¸âƒ£ å°çµ\n",
      "\n",
      "1. **å®‰è£ VSCode + Dev Containers**  \n",
      "2. **Reopen in Container** â†’ é€²å…¥å®¹å™¨ Bash  \n",
      "3. **è¨­å®š `launch.json`** â†’ é–‹å§‹é™¤éŒ¯  \n",
      "4. **è™•ç†å¸¸è¦‹éŒ¯èª¤**ï¼ˆuserlandâ€‘proxyã€iptablesã€firewalldï¼‰  \n",
      "5. **å¯é¸ï¼šå•Ÿå‹• Docker Swarm** â†’ é€²è¡Œå¤šç¯€é»æ¸¬è©¦  \n",
      "\n",
      "åªè¦æŒ‰ä¸Šè¿°æ­¥é©Ÿæ“ä½œï¼Œä½ å°±èƒ½åœ¨ VSCode å…§éƒ¨é †åˆ©å•Ÿå‹•ã€é™¤éŒ¯ä¸¦æ¸¬è©¦ Moby å°ˆæ¡ˆã€‚ç¥é–‹ç™¼é †åˆ© ğŸš€\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You >  å¦‚ä½•å¡«å¯«launch.json in vscode, launch.json only\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================route_question\n",
      "content='DevEnvAgent' additional_kwargs={} response_metadata={'model': 'gpt-oss:20b', 'created_at': '2025-09-14T13:18:33.836950809Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1268998446, 'load_duration': 82414253, 'prompt_eval_count': 252, 'prompt_eval_duration': 126462759, 'eval_count': 68, 'eval_duration': 1058099943, 'model_name': 'gpt-oss:20b'} id='run--36f1b7c0-8367-4b68-8331-02801b8a086f-0' usage_metadata={'input_tokens': 252, 'output_tokens': 68, 'total_tokens': 320}\n",
      "=================route_question\n",
      "DevEnvAgent\n",
      "[Router â†’ DevEnvAgent]\n",
      "```json\n",
      "{\n",
      "    \"version\": \"0.2.0\",\n",
      "    \"configurations\": [\n",
      "        {\n",
      "            \"name\": \"Launch Package\",\n",
      "            \"type\": \"go\",\n",
      "            \"request\": \"launch\",\n",
      "            \"mode\": \"auto\",\n",
      "            \"program\": \"${fileDirname}\"\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "```\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You >  how to run a unit test\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================route_question\n",
      "content='TestingAgent' additional_kwargs={} response_metadata={'model': 'gpt-oss:20b', 'created_at': '2025-09-14T13:19:20.096410663Z', 'done': True, 'done_reason': 'stop', 'total_duration': 647488399, 'load_duration': 81606237, 'prompt_eval_count': 247, 'prompt_eval_duration': 123419498, 'eval_count': 29, 'eval_duration': 440470106, 'model_name': 'gpt-oss:20b'} id='run--10e6eec3-c24a-4930-a9d0-e4a33ea363b8-0' usage_metadata={'input_tokens': 247, 'output_tokens': 29, 'total_tokens': 276}\n",
      "=================route_question\n",
      "TestingAgent\n",
      "[Router â†’ TestingAgent]\n",
      "è¦åŸ·è¡Œå–®å…ƒæ¸¬è©¦ï¼Œè«‹ä¾ç…§ **TESTING.md** ä¸­çš„èªªæ˜ï¼š\n",
      "\n",
      "1. åœ¨å°ˆæ¡ˆæ ¹ç›®éŒ„åŸ·è¡Œ  \n",
      "   ```bash\n",
      "   make test-unit\n",
      "   ```\n",
      "2. æ¸¬è©¦çµæœæœƒé¡¯ç¤ºåœ¨ **TEST RESULT** é¢æ¿ä¸­ã€‚  \n",
      "\n",
      "é€™å°±æ˜¯åŸ·è¡Œå–®å…ƒæ¸¬è©¦çš„å®Œæ•´æµç¨‹ã€‚\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You >  testæœ‰å¹¾ç¨®ç¨®é¡ï¼Ÿ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================route_question\n",
      "content='TestingAgent' additional_kwargs={} response_metadata={'model': 'gpt-oss:20b', 'created_at': '2025-09-14T13:19:41.355620904Z', 'done': True, 'done_reason': 'stop', 'total_duration': 884866037, 'load_duration': 83092685, 'prompt_eval_count': 248, 'prompt_eval_duration': 121132874, 'eval_count': 44, 'eval_duration': 678632751, 'model_name': 'gpt-oss:20b'} id='run--80086df1-f5c6-42e8-a812-bbab062f7b48-0' usage_metadata={'input_tokens': 248, 'output_tokens': 44, 'total_tokens': 292}\n",
      "=================route_question\n",
      "TestingAgent\n",
      "[Router â†’ TestingAgent]\n",
      "åœ¨ Moby Project çš„æ¸¬è©¦æµç¨‹ä¸­ï¼Œä¸»è¦åˆ†ç‚º **å…©ç¨®æ¸¬è©¦é¡å‹**ï¼š\n",
      "\n",
      "| é¡å‹ | ä¸»è¦ç”¨é€” | ç›¸é—œæŒ‡ä»¤æˆ–è¨­å®š |\n",
      "|------|----------|---------------|\n",
      "| **å–®å…ƒæ¸¬è©¦ (Unit Test)** | é©—è­‰å–®ä¸€æ¨¡çµ„æˆ–å‡½å¼çš„æ­£ç¢ºæ€§ | `make test-unit` |\n",
      "| **æ•´åˆæ¸¬è©¦ (Integration Test)** | é©—è­‰å¤šå€‹æ¨¡çµ„ä¹‹é–“çš„å”åŒå·¥ä½œï¼Œæˆ–æ•´é«”åŠŸèƒ½çš„æ­£ç¢ºæ€§ | `make test-integration`ï¼ˆéœ€å…ˆè¨­å®šå®¹å™¨ã€å®‰è£ iptables ç­‰ï¼‰ |\n",
      "\n",
      "> é™¤äº†ä¸Šè¿°å…©å¤§é¡å‹ï¼Œæ–‡ä»¶ä¸­é‚„èªªæ˜äº†å¦‚ä½•åœ¨ Debug æ¨¡å¼ä¸‹åŸ·è¡Œå–®ä¸€å–®å…ƒæ¸¬è©¦ï¼Œä»¥åŠå¦‚ä½•é‡å°ç‰¹å®šæ•´åˆæ¸¬è©¦æ¡ˆä¾‹ï¼ˆå¦‚ `TestBridgeICC/IPv4_non-internal_network`ï¼‰é€²è¡Œå–®ä¸€æ•´åˆæ¸¬è©¦ï¼Œä½†é€™äº›ä»å±¬æ–¼ã€Œå–®å…ƒæ¸¬è©¦ã€æˆ–ã€Œæ•´åˆæ¸¬è©¦ã€çš„ç¯„ç–‡ã€‚  \n",
      "\n",
      "å› æ­¤ï¼ŒMoby Project çš„æ¸¬è©¦æ–¹å¼ç¸½å…±å¯æ­¸ç´ç‚º **å…©ç¨®ä¸»è¦é¡å‹**ï¼šå–®å…ƒæ¸¬è©¦å’Œæ•´åˆæ¸¬è©¦ã€‚\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You >  æ•´åˆæ¸¬è©¦ (Integration Test)*æ­¥é©Ÿ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================route_question\n",
      "content='TestingAgent' additional_kwargs={} response_metadata={'model': 'gpt-oss:20b', 'created_at': '2025-09-14T13:22:20.483600577Z', 'done': True, 'done_reason': 'stop', 'total_duration': 963272972, 'load_duration': 78978968, 'prompt_eval_count': 252, 'prompt_eval_duration': 124372806, 'eval_count': 49, 'eval_duration': 757885207, 'model_name': 'gpt-oss:20b'} id='run--f19b12a0-075e-4eea-aa75-8fac4983bac5-0' usage_metadata={'input_tokens': 252, 'output_tokens': 49, 'total_tokens': 301}\n",
      "=================route_question\n",
      "TestingAgent\n",
      "[Router â†’ TestingAgent]\n",
      "**æ•´åˆæ¸¬è©¦ï¼ˆIntegration Testï¼‰æ­¥é©Ÿ**\n",
      "\n",
      "| æ­¥é©Ÿ | æ“ä½œèªªæ˜ | ç›¸é—œæŒ‡ä»¤ / åƒæ•¸ |\n",
      "|------|----------|-----------------|\n",
      "| 1 | **å»ºç«‹å®¹å™¨ä¸¦åŸ·è¡Œ Docker Daemon** | å…ˆåœ¨æ¸¬è©¦ç’°å¢ƒä¸­å•Ÿå‹•ä¸€å€‹å®¹å™¨ï¼Œä¸¦åœ¨å®¹å™¨å…§åŸ·è¡Œ `dockerd`ã€‚ |\n",
      "| 2 | **ä¿®æ”¹ `hack/make.sh`** | åœ¨ `hack/make.sh` è£¡æ–°å¢ä»¥ä¸‹å…©è¡Œï¼Œç¢ºä¿ iptables ç‰ˆæœ¬æ­£ç¢ºï¼š<br>```bash<br>apt remove iptables -y<br>apt install iptables -y<br>``` |\n",
      "| 3 | **åŸ·è¡Œæ•´åˆæ¸¬è©¦** | ```bash<br>make test-integration<br>``` |\n",
      "| 4 | **ï¼ˆå–®ä¸€æ¸¬è©¦ï¼‰è¨­å®šç’°å¢ƒè®Šæ•¸** | è‹¥åªæƒ³è·‘å–®ä¸€æ¸¬è©¦ï¼Œå…ˆè¨­å®šç’°å¢ƒè®Šæ•¸ï¼š<br>```bash<br>export TESTFLAGS='-test.run TestBridgeICC/IPv4_non-internal_network'<br>export TEST_INTEGRATION_DIR=./integration/networking<br>```<br>ç„¶å¾Œå†åŸ·è¡Œ `make test-integration`ã€‚ |\n",
      "| 5 | **æŸ¥çœ‹æ¸¬è©¦çµæœ** | æ¸¬è©¦çµæœæœƒé¡¯ç¤ºåœ¨ **TEST RESULT** é¢æ¿ã€‚<br>ï¼ˆåƒè€ƒé™„åœ–ï¼š`image.png`ã€`scaled-1680-/JuBimage.png` ç­‰ï¼‰ |\n",
      "\n",
      "> **å‚™è¨»**  \n",
      "> - æ­¥é©Ÿ 2 çš„ iptables é‡æ–°å®‰è£æ˜¯ç‚ºäº†é¿å…æ¸¬è©¦ç’°å¢ƒä¸­ iptables ç‰ˆæœ¬ä¸ä¸€è‡´é€ æˆçš„éŒ¯èª¤ã€‚  \n",
      "> - æ­¥é©Ÿ 4 çš„ç’°å¢ƒè®Šæ•¸ `TESTFLAGS` å…è¨±ä½ åªè·‘æŒ‡å®šçš„æ¸¬è©¦å‡½å¼ï¼Œ`TEST_INTEGRATION_DIR` æŒ‡å®šæ¸¬è©¦ç›®éŒ„ã€‚  \n",
      "> - æ‰€æœ‰æ¸¬è©¦çµæœå‡æœƒåœ¨ **TEST RESULT** é¢æ¿ä¸­å‘ˆç¾ï¼Œä¸¦ä¸”æ–‡ä»¶ä¸­å·²é™„ä¸Šç›¸æ‡‰çš„æˆªåœ–ä¾›åƒè€ƒã€‚\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You >  æˆ‘æƒ³è¦è·‘moby project, ç”¨å“ªç¨®ideå¥½ï¼Ÿ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================route_question\n",
      "content='DevEnvAgent' additional_kwargs={} response_metadata={'model': 'gpt-oss:20b', 'created_at': '2025-09-14T13:24:07.191555624Z', 'done': True, 'done_reason': 'stop', 'total_duration': 835486441, 'load_duration': 81871387, 'prompt_eval_count': 255, 'prompt_eval_duration': 121337921, 'eval_count': 41, 'eval_duration': 630336970, 'model_name': 'gpt-oss:20b'} id='run--f5d0490f-c849-44ef-9b80-894862349bdf-0' usage_metadata={'input_tokens': 255, 'output_tokens': 41, 'total_tokens': 296}\n",
      "=================route_question\n",
      "DevEnvAgent\n",
      "[Router â†’ DevEnvAgent]\n",
      "æ ¹æ“šæ–‡ä»¶ä¸­çš„èªªæ˜ï¼Œ**VSCodeï¼ˆæ­é… Dev Containers æ“´å……å¥—ä»¶ï¼‰** æ˜¯è·‘ Moby Project æœ€å¸¸ç”¨ä¸”æœ€å®Œæ•´çš„é–‹ç™¼ç’°å¢ƒã€‚  \n",
      "\n",
      "### ç‚ºä»€éº¼é¸æ“‡ VSCode + Dev Containersï¼Ÿ\n",
      "\n",
      "| ç‰¹è‰² | èªªæ˜ |\n",
      "|------|------|\n",
      "| **å®¹å™¨åŒ–é–‹ç™¼ç’°å¢ƒ** | Dev Containers è®“ä½ åœ¨ VSCode å…§ç›´æ¥é€²å…¥ Moby çš„é–‹ç™¼å®¹å™¨ï¼Œç’°å¢ƒèˆ‡ CI ä¸€è‡´ã€‚ |\n",
      "| **å³æ™‚ Debug** | åªè¦åœ¨ `launch.json` è¨­å®šå¥½ `go` èª¿è©¦é…ç½®ï¼Œå³å¯åœ¨å®¹å™¨å…§ç›´æ¥æ–·é»ã€å–®æ­¥åŸ·è¡Œã€‚ |\n",
      "| **å®Œæ•´çš„ Docker æ”¯æ´** | VSCode å…§å»º Docker æ“´å……å¥—ä»¶ï¼Œå¯ç›´æ¥ç®¡ç†å®¹å™¨ã€é¡åƒã€Swarmã€‚ |\n",
      "| **è·¨å¹³å°** | ç„¡è«–ä½ åœ¨ Windowsã€macOS æˆ– Linux ä¸Šï¼Œéƒ½èƒ½ä»¥ç›¸åŒæ–¹å¼é–‹ç™¼ã€‚ |\n",
      "\n",
      "### å¿«é€Ÿä¸Šæ‰‹æµç¨‹ï¼ˆåƒè€ƒæ–‡ä»¶ï¼‰\n",
      "\n",
      "1. **å®‰è£ VSCode**  \n",
      "   - ä¸‹è¼‰ä¸¦å®‰è£ VSCodeã€‚  \n",
      "2. **å®‰è£ Dev Containers æ“´å……å¥—ä»¶**  \n",
      "   - åœ¨ VSCode Marketplace æœå°‹ `Dev Containers`ï¼Œé»æ“Šå®‰è£ã€‚  \n",
      "3. **é‡å•Ÿ VSCode**  \n",
      "   - é‡æ–°å•Ÿå‹•å¾Œï¼ŒVSCode æœƒé¡¯ç¤ºã€Œcontainer bashã€æç¤ºã€‚  \n",
      "4. **Reopen in Container**  \n",
      "   - åœ¨ VSCode æŒ‡ä»¤é¢æ¿ï¼ˆ`Ctrl+Shift+P`ï¼‰è¼¸å…¥ `Reopen in Container`ï¼Œè®“å°ˆæ¡ˆåœ¨å®¹å™¨å…§é–‹å•Ÿã€‚  \n",
      "5. **è¨­å®š `launch.json`**  \n",
      "   ```json\n",
      "   {\n",
      "       \"version\": \"0.2.0\",\n",
      "       \"configurations\": [\n",
      "           {\n",
      "               \"name\": \"Launch Package\",\n",
      "               \"type\": \"go\",\n",
      "               \"request\": \"launch\",\n",
      "               \"mode\": \"auto\",\n",
      "               \"program\": \"${fileDirname}\"\n",
      "           }\n",
      "       ]\n",
      "   }\n",
      "   ```\n",
      "6. **è§£æ±ºå¸¸è¦‹éŒ¯èª¤**  \n",
      "   - è‹¥å‡ºç¾ `error: Invalid userlane-proxy-path`ï¼Œè«‹åœ¨ `/etc/docker/daemon.json` è¨­å®š `userland-proxy: false`ã€‚  \n",
      "   - è‹¥å‡ºç¾ `modprobe bridge br_netfilter failed`ï¼Œè«‹é‡æ–°å®‰è£ `iptables`ï¼š  \n",
      "     ```bash\n",
      "     apt remove iptables\n",
      "     apt install iptables\n",
      "     ```\n",
      "7. **åŸ·è¡Œ Debug**  \n",
      "   - åœ¨ VSCode ä¸­æŒ‰ä¸‹ `F5` æˆ–é»æ“Šã€ŒRunã€æŒ‰éˆ•ï¼Œå³å¯é€²å…¥ Debug æ¨¡å¼ã€‚  \n",
      "\n",
      "> **å‚™è¨»**  \n",
      "> - è‹¥ä½ æƒ³åœ¨ Dev Container å…§è·‘å®Œæ•´çš„ Docker Engineï¼ˆ`dockerd`ï¼‰ï¼Œè«‹åƒè€ƒã€ŒRun Moby inside Dev Containerã€çš„æ­¥é©Ÿï¼š  \n",
      ">   ```bash\n",
      ">   hack/make.sh binary\n",
      ">   make install\n",
      ">   dockerd -D\n",
      ">   ```  \n",
      "> - è‹¥ä½ æƒ³åœ¨ VSCode å…§å•Ÿå‹• Docker Swarm Managerï¼Œè«‹åƒè€ƒã€ŒRun Moby Project and Create Docker Swarm Manager in VSCodeã€çš„èªªæ˜ã€‚  \n",
      "\n",
      "### çµè«–\n",
      "\n",
      "- **æœ€ä½³é¸æ“‡**ï¼šVSCode + Dev Containers  \n",
      "- **å„ªé»**ï¼šå®¹å™¨åŒ–ä¸€è‡´ç’°å¢ƒã€å³æ™‚ Debugã€è·¨å¹³å°ã€è±å¯Œæ“´å……å¥—ä»¶ã€‚  \n",
      "\n",
      "å¦‚æœä½ å·²ç¶“ç†Ÿæ‚‰ VSCodeï¼Œç›´æ¥ä½¿ç”¨ä¸Šè¿°æµç¨‹å³å¯å¿«é€Ÿé–‹å§‹é–‹ç™¼ Moby Projectã€‚ç¥é–‹ç™¼é †åˆ©ï¼\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You >  error: Invalid userlane-proxy-path é€™å€‹æ˜¯å“ªå€‹agent?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================route_question\n",
      "content='IssueAgent' additional_kwargs={} response_metadata={'model': 'gpt-oss:20b', 'created_at': '2025-09-14T13:25:11.132298002Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1668767150, 'load_duration': 82351201, 'prompt_eval_count': 256, 'prompt_eval_duration': 120405838, 'eval_count': 94, 'eval_duration': 1463940173, 'model_name': 'gpt-oss:20b'} id='run--4764f7fd-c523-456e-a81e-d76a68679c7f-0' usage_metadata={'input_tokens': 256, 'output_tokens': 94, 'total_tokens': 350}\n",
      "=================route_question\n",
      "IssueAgent\n",
      "[Router â†’ IssueAgent]\n",
      "æŠ±æ­‰ï¼Œæ ¹æ“šç›®å‰æä¾›çš„æ–‡ä»¶å…§å®¹ï¼Œæˆ‘æ‰¾ä¸åˆ°ä»»ä½•é—œæ–¼ã€ŒInvalid userlane-proxy-pathã€çš„ Issue æˆ–ç›¸é—œ agentã€‚è‹¥æ‚¨æœ‰å…¶ä»–ç›¸é—œè³‡è¨Šæˆ–æƒ³æŸ¥è©¢å…¶ä»–å·²åˆ—å‡ºçš„ Issueï¼Œè«‹éš¨æ™‚å‘Šè¨´æˆ‘ã€‚\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You >  é–‹ç™¼çš„æ™‚å¾Œç¢°åˆ°error: Invalid userlane-proxy-path \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================route_question\n",
      "content='DevEnvAgent' additional_kwargs={} response_metadata={'model': 'gpt-oss:20b', 'created_at': '2025-09-14T13:25:39.86829392Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2175119468, 'load_duration': 84099209, 'prompt_eval_count': 256, 'prompt_eval_duration': 120962317, 'eval_count': 126, 'eval_duration': 1967920663, 'model_name': 'gpt-oss:20b'} id='run--2623a3ec-4292-42dc-bba4-4bbf5f9ce305-0' usage_metadata={'input_tokens': 256, 'output_tokens': 126, 'total_tokens': 382}\n",
      "=================route_question\n",
      "DevEnvAgent\n",
      "[Router â†’ DevEnvAgent]\n",
      "**è§£æ±ºã€Œerror: Invalid userlaneâ€‘proxyâ€‘pathã€çš„æ­¥é©Ÿ**\n",
      "\n",
      "é€™å€‹éŒ¯èª¤è¨Šæ¯è¡¨ç¤º Docker çš„ `userland-proxy` åŠŸèƒ½å·²ç¶“å•Ÿç”¨ï¼Œä½†ç³»çµ±æ‰¾ä¸åˆ°å°æ‡‰çš„åŸ·è¡Œæª”è·¯å¾‘ã€‚  \n",
      "åœ¨ Moby çš„é–‹ç™¼ç’°å¢ƒä¸­ï¼Œæœ€å¸¸è¦‹çš„åšæ³•æ˜¯ **é—œé–‰** `userland-proxy`ï¼Œæˆ–æ˜¯æ˜ç¢ºæŒ‡å®šå…¶è·¯å¾‘ã€‚\n",
      "\n",
      "---\n",
      "\n",
      "### 1. å…ˆç¢ºèª `/etc/docker/daemon.json`\n",
      "\n",
      "```bash\n",
      "sudo nano /etc/docker/daemon.json\n",
      "```\n",
      "\n",
      "å¦‚æœæª”æ¡ˆä¸å­˜åœ¨ï¼Œå…ˆå»ºç«‹ä¸€å€‹ã€‚  \n",
      "åœ¨æª”æ¡ˆä¸­åŠ å…¥ï¼ˆæˆ–ç¢ºèªå·²å­˜åœ¨ï¼‰ä»¥ä¸‹è¨­å®šï¼š\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"runtimes\": {\n",
      "        \"crun\": {\n",
      "            \"path\": \"/usr/local/bin/crun\"\n",
      "        }\n",
      "    },\n",
      "    \"features\": {\n",
      "        \"containerd-snapshotter\": false\n",
      "    },\n",
      "    \"userland-proxy\": false,\n",
      "    \"iptables\": false\n",
      "}\n",
      "```\n",
      "\n",
      "> **èªªæ˜**  \n",
      "> - `userland-proxy: false` æœƒé—œé–‰è©²åŠŸèƒ½ï¼Œé¿å…éœ€è¦æŒ‡å®šè·¯å¾‘ã€‚  \n",
      "> - `iptables: false` ä¹Ÿå¸¸åœ¨é–‹ç™¼ç’°å¢ƒä¸­é—œé–‰ï¼Œä»¥æ¸›å°‘é¡å¤–çš„ç¶²è·¯è¨­å®šã€‚\n",
      "\n",
      "> å¦‚æœä½ æƒ³ä¿ç•™ `userland-proxy`ï¼Œå¯ä»¥æ”¹æˆ  \n",
      "> ```json\n",
      "> \"userland-proxy-path\": \"/usr/local/bin/userland-proxy\"\n",
      "> ```  \n",
      "> ä½†é€™éœ€è¦ä½ ç¢ºå®šè©²åŸ·è¡Œæª”å·²å®‰è£åœ¨è©²è·¯å¾‘ã€‚\n",
      "\n",
      "---\n",
      "\n",
      "### 2. é‡æ–°å•Ÿå‹• Docker\n",
      "\n",
      "```bash\n",
      "sudo systemctl restart docker\n",
      "```\n",
      "\n",
      "æˆ–åœ¨ Dev Container å…§ï¼š\n",
      "\n",
      "```bash\n",
      "dockerd -D\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### 3. é©—è­‰\n",
      "\n",
      "```bash\n",
      "docker info | grep -i userland\n",
      "```\n",
      "\n",
      "æ‡‰è©²é¡¯ç¤º `Userland Proxy: false`ï¼ˆæˆ–å·²é—œé–‰ï¼‰ã€‚\n",
      "\n",
      "---\n",
      "\n",
      "### 4. å…¶ä»–å¯èƒ½çš„éŒ¯èª¤\n",
      "\n",
      "- **iptables æœªå®‰è£**  \n",
      "  è‹¥é‚„å‡ºç¾ `Running modprobe bridge br_netfilter failed`ï¼Œè«‹å…ˆé‡æ–°å®‰è£ iptablesï¼š\n",
      "\n",
      "  ```bash\n",
      "  sudo apt remove iptables\n",
      "  sudo apt install iptables\n",
      "  ```\n",
      "\n",
      "- **ç¢ºä¿ VSCode çš„ Dev Containers å·²æ­£ç¢ºå•Ÿå‹•**  \n",
      "  é‡æ–°é–‹å•Ÿå®¹å™¨ï¼š`Reopen in Container`ï¼Œä¸¦ç¢ºèª `launch.json` çš„è¨­å®šæ­£ç¢ºã€‚\n",
      "\n",
      "---\n",
      "\n",
      "#### å°çµ\n",
      "\n",
      "- **é—œé–‰** `userland-proxy`ï¼ˆæœ€ç°¡å–®ï¼‰æˆ–  \n",
      "- **æŒ‡å®š** `userland-proxy-path`ï¼ˆè‹¥å¿…é ˆä½¿ç”¨ï¼‰ã€‚\n",
      "\n",
      "å®Œæˆä¸Šè¿°æ­¥é©Ÿå¾Œï¼Œé–‹ç™¼ç’°å¢ƒä¸­çš„ `error: Invalid userlane-proxy-path` æ‡‰è©²å°±æœƒæ¶ˆå¤±ã€‚ç¥é–‹ç™¼é †åˆ©ï¼\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You >  code trace : VSCode è¿”å›ä¸Šä¸€å€‹æª¢è¦–çš„å¿«æ·éµ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================route_question\n",
      "content='DevEnvAgent' additional_kwargs={} response_metadata={'model': 'gpt-oss:20b', 'created_at': '2025-09-14T13:30:13.129131153Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1399012130, 'load_duration': 91811583, 'prompt_eval_count': 255, 'prompt_eval_duration': 120832930, 'eval_count': 76, 'eval_duration': 1184357692, 'model_name': 'gpt-oss:20b'} id='run--6233bc4d-0b3d-4f47-b381-42ac47c0d53d-0' usage_metadata={'input_tokens': 255, 'output_tokens': 76, 'total_tokens': 331}\n",
      "=================route_question\n",
      "DevEnvAgent\n",
      "[Router â†’ DevEnvAgent]\n",
      "æŠ±æ­‰ï¼Œæˆ‘ç›®å‰åªèƒ½æ ¹æ“šæ‚¨æä¾›çš„æ–‡ä»¶å…§å®¹å›ç­”å•é¡Œã€‚æ–‡ä»¶ä¸­ä¸¦æœªåŒ…å« VSCode çš„å¿«æ·éµè³‡è¨Šï¼Œå› æ­¤ç„¡æ³•æä¾›ã€Œè¿”å›ä¸Šä¸€å€‹æª¢è¦–ã€çš„å¿«æ·éµã€‚è‹¥æ‚¨éœ€è¦æ­¤è³‡è¨Šï¼Œå»ºè­°æŸ¥é–± VSCode å®˜æ–¹æ–‡ä»¶æˆ–è¨­å®šé é¢ã€‚\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You >  é–‹ç™¼çš„æ™‚å€™é‡åˆ° error: Invalid userlane-proxy-path: userland-proxy is enabled, but userland-proxy-path is not set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================route_question\n",
      "content='DevEnvAgent' additional_kwargs={} response_metadata={'model': 'gpt-oss:20b', 'created_at': '2025-09-14T13:41:40.938459643Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2897454124, 'load_duration': 77779212, 'prompt_eval_count': 273, 'prompt_eval_duration': 145257615, 'eval_count': 171, 'eval_duration': 2671951788, 'model_name': 'gpt-oss:20b'} id='run--63c35fe2-cc97-4e32-bb98-a9a86a8e9059-0' usage_metadata={'input_tokens': 273, 'output_tokens': 171, 'total_tokens': 444}\n",
      "=================route_question\n",
      "DevEnvAgent\n",
      "[Router â†’ DevEnvAgent]\n",
      "**è§£æ±ºã€ŒInvalid userlaneâ€‘proxyâ€‘path: userlandâ€‘proxy is enabled, but userlandâ€‘proxyâ€‘path is not setã€çš„æ­¥é©Ÿ**\n",
      "\n",
      "é€™å€‹éŒ¯èª¤æ˜¯å› ç‚º Docker çš„ `userland-proxy` åŠŸèƒ½è¢«å•Ÿç”¨ï¼Œä½†æ²’æœ‰æŒ‡å®šå°æ‡‰çš„åŸ·è¡Œæª”è·¯å¾‘ã€‚æœ€ç°¡å–®çš„åšæ³•æ˜¯ **é—œé–‰** `userland-proxy`ï¼Œæˆ–æ˜¯æ˜ç¢ºæŒ‡å®šè·¯å¾‘ã€‚ä»¥ä¸‹æ˜¯å…©ç¨®å¸¸è¦‹çš„ä¿®æ­£æ–¹å¼ï¼Œè«‹ä¾ç…§ä½ ç›®å‰çš„é–‹ç™¼ç’°å¢ƒé¸æ“‡å…¶ä¸­ä¸€ç¨®ï¼š\n",
      "\n",
      "---\n",
      "\n",
      "### 1. ç›´æ¥é—œé–‰ `userland-proxy`\n",
      "\n",
      "1. **ç·¨è¼¯** `/etc/docker/daemon.json`ï¼ˆå¦‚æœæª”æ¡ˆä¸å­˜åœ¨ï¼Œå…ˆå»ºç«‹ä¸€å€‹ï¼‰  \n",
      "   ```bash\n",
      "   sudo nano /etc/docker/daemon.json\n",
      "   ```\n",
      "\n",
      "2. åœ¨ JSON å…§å®¹ä¸­åŠ å…¥æˆ–ä¿®æ”¹ä»¥ä¸‹è¨­å®šï¼š\n",
      "   ```json\n",
      "   {\n",
      "       \"userland-proxy\": false,\n",
      "       \"iptables\": false\n",
      "   }\n",
      "   ```\n",
      "   > *èªªæ˜*  \n",
      "   * `userland-proxy: false` æœƒæŠŠé€™å€‹åŠŸèƒ½é—œé–‰ï¼Œé¿å… Docker éœ€è¦ `userland-proxy-path`ã€‚  \n",
      "   * `iptables: false` ä¹Ÿå¸¸èˆ‡æ­¤è¨­å®šä¸€èµ·ä½¿ç”¨ï¼Œé¿å…åœ¨å®¹å™¨å…§éƒ¨å†å•Ÿç”¨ iptablesã€‚\n",
      "\n",
      "3. **é‡å•Ÿ Docker** ä»¥å¥—ç”¨è¨­å®šï¼š\n",
      "   ```bash\n",
      "   sudo systemctl restart docker\n",
      "   ```\n",
      "\n",
      "4. å†æ¬¡åŸ·è¡Œä½ ä¹‹å‰çš„æŒ‡ä»¤ï¼ŒéŒ¯èª¤æ‡‰è©²å·²æ¶ˆå¤±ã€‚\n",
      "\n",
      "---\n",
      "\n",
      "### 2. æŒ‡å®š `userland-proxy-path`\n",
      "\n",
      "å¦‚æœä½ éœ€è¦ä¿ç•™ `userland-proxy`ï¼ˆä¾‹å¦‚æŸäº›èˆŠç‰ˆå®¹å™¨éœ€è¦å®ƒï¼‰ï¼Œå¯ä»¥ç›´æ¥å‘Šè¨´ Docker å®ƒçš„åŸ·è¡Œæª”è·¯å¾‘ï¼š\n",
      "\n",
      "1. **ç¢ºèª** `userland-proxy` å¯åŸ·è¡Œæª”ä½ç½®ï¼ˆé€šå¸¸åœ¨ `/usr/local/bin/userland-proxy` æˆ– `/usr/bin/userland-proxy`ï¼‰ã€‚  \n",
      "   ```bash\n",
      "   which userland-proxy\n",
      "   ```\n",
      "\n",
      "2. **ç·¨è¼¯** `/etc/docker/daemon.json`ï¼ŒåŠ å…¥ `userland-proxy-path`ï¼š\n",
      "   ```json\n",
      "   {\n",
      "       \"userland-proxy\": true,\n",
      "       \"userland-proxy-path\": \"/usr/local/bin/userland-proxy\",\n",
      "       \"iptables\": false\n",
      "   }\n",
      "   ```\n",
      "\n",
      "3. **é‡å•Ÿ Docker**ï¼š\n",
      "   ```bash\n",
      "   sudo systemctl restart docker\n",
      "   ```\n",
      "\n",
      "4. å†æ¬¡åŸ·è¡Œä½ çš„ç¨‹å¼ï¼ŒéŒ¯èª¤æ‡‰è©²å·²è§£æ±ºã€‚\n",
      "\n",
      "---\n",
      "\n",
      "### 3. åœ¨ VSCode Dev Container å…§éƒ¨\n",
      "\n",
      "å¦‚æœä½ æ˜¯åœ¨ VSCode çš„ Dev Container å…§éƒ¨é‡åˆ°æ­¤éŒ¯èª¤ï¼Œè«‹ç¢ºä¿ï¼š\n",
      "\n",
      "1. **å®¹å™¨å…§éƒ¨**ä¹Ÿæœ‰ç›¸åŒçš„ `/etc/docker/daemon.json` è¨­å®šã€‚  \n",
      "   - ä½ å¯ä»¥åœ¨å®¹å™¨çš„ `bash` å…§éƒ¨ç›´æ¥ç·¨è¼¯ `/etc/docker/daemon.json`ï¼Œæˆ–åœ¨ `devcontainer.json` çš„ `postCreateCommand` è£¡é¢å¯«å…¥è¨­å®šæª”ã€‚\n",
      "\n",
      "2. **é‡å•Ÿå®¹å™¨å…§éƒ¨çš„ Docker**ï¼ˆå¦‚æœä½ åœ¨å®¹å™¨å…§éƒ¨è·‘ `dockerd`ï¼‰ï¼š\n",
      "   ```bash\n",
      "   dockerd -D\n",
      "   ```\n",
      "\n",
      "3. **ç¢ºèª iptables å·²å®‰è£**ï¼ˆè‹¥ä½ ä½¿ç”¨ `iptables: false`ï¼Œä»éœ€å®‰è£ iptables ä»¥é¿å…å…¶ä»–éŒ¯èª¤ï¼‰ï¼š\n",
      "   ```bash\n",
      "   apt remove iptables\n",
      "   apt install iptables\n",
      "   ```\n",
      "\n",
      "---\n",
      "\n",
      "### å°çµ\n",
      "\n",
      "| æ–¹æ¡ˆ | ä½•æ™‚ä½¿ç”¨ | ä¸»è¦è¨­å®š |\n",
      "|------|----------|----------|\n",
      "| é—œé–‰ `userland-proxy` | ä½ ä¸éœ€è¦æ­¤åŠŸèƒ½ | `\"userland-proxy\": false` |\n",
      "| æŒ‡å®š `userland-proxy-path` | ä½ éœ€è¦ä¿ç•™æ­¤åŠŸèƒ½ | `\"userland-proxy-path\": \"/path/to/userland-proxy\"` |\n",
      "\n",
      "åªè¦æŠŠä¸Šè¿°è¨­å®šå¯«é€² `/etc/docker/daemon.json`ï¼Œé‡å•Ÿ Dockerï¼ˆæˆ–å®¹å™¨å…§éƒ¨çš„ `dockerd`ï¼‰ï¼Œå°±èƒ½æ¶ˆé™¤ã€ŒInvalid userlaneâ€‘proxyâ€‘pathã€çš„éŒ¯èª¤ã€‚ç¥é–‹ç™¼é †åˆ©ï¼\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You >  æˆ‘æƒ³è¦è·‘debug mode moby project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================route_question\n",
      "content='DevEnvAgent' additional_kwargs={} response_metadata={'model': 'gpt-oss:20b', 'created_at': '2025-09-14T13:43:18.405644041Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1579931006, 'load_duration': 83032606, 'prompt_eval_count': 250, 'prompt_eval_duration': 122727509, 'eval_count': 88, 'eval_duration': 1371472302, 'model_name': 'gpt-oss:20b'} id='run--0c576023-da99-42cd-9c4a-e83386c53b2e-0' usage_metadata={'input_tokens': 250, 'output_tokens': 88, 'total_tokens': 338}\n",
      "=================route_question\n",
      "DevEnvAgent\n",
      "[Router â†’ DevEnvAgent]\n",
      "ä»¥ä¸‹æ˜¯ä½ å¯ä»¥ç›´æ¥ç…§è‘—åŸ·è¡Œçš„æ­¥é©Ÿï¼Œè®“ Moby Project åœ¨ VSCode å…§ä»¥ **debug mode** åŸ·è¡Œã€‚  \n",
      "ï¼ˆæ‰€æœ‰è³‡è¨Šå‡ä¾†è‡ªä½ æä¾›çš„æ–‡ä»¶ï¼Œè«‹ä¾ç…§é †åºæ“ä½œï¼Œè‹¥é‡åˆ°éŒ¯èª¤è«‹åƒè€ƒå°æ‡‰çš„ã€Œå¯èƒ½éŒ¯èª¤ã€èªªæ˜ã€‚ï¼‰\n",
      "\n",
      "---\n",
      "\n",
      "## 1ï¸âƒ£ å…ˆç¢ºå®šé–‹ç™¼ç’°å¢ƒå·²å®‰è£å¥½\n",
      "\n",
      "| æ­¥é©Ÿ | å…§å®¹ | åƒè€ƒ |\n",
      "|------|------|------|\n",
      "| 1 | **å®‰è£ VSCode**ï¼ˆè‹¥å°šæœªå®‰è£ï¼‰ |  |\n",
      "| 2 | **å®‰è£ VSCode æ’ä»¶ï¼šDev Containers** |  |\n",
      "| 3 | **é‡å•Ÿ VSCode**ï¼Œç¢ºèªç•«é¢é¡¯ç¤ºã€Œcontainer bashã€ |  |\n",
      "| 4 | **åœ¨ VSCode æŒ‡ä»¤é¢æ¿ï¼ˆCtrl+Shift+Pï¼‰è¼¸å…¥** `Reopen in Container` |  |\n",
      "\n",
      "> é€™ä¸€æ­¥æœƒæŠŠæ•´å€‹å°ˆæ¡ˆæ‰“åŒ…é€²ä¸€å€‹ Docker å®¹å™¨ï¼Œä¸¦åœ¨å®¹å™¨å…§é–‹å•Ÿ VSCodeã€‚  \n",
      "> è‹¥ä½ å·²ç¶“åœ¨å®¹å™¨å…§ï¼Œç›´æ¥è·³åˆ°ç¬¬ 5 æ­¥ã€‚\n",
      "\n",
      "---\n",
      "\n",
      "## 2ï¸âƒ£ è¨­å®š VSCode çš„ `launch.json`\n",
      "\n",
      "1. åœ¨ VSCode å·¦å´ã€ŒRunã€é¢æ¿ï¼ˆæˆ– `Ctrl+Shift+D`ï¼‰é»æ“Š **Create a launch.json file**ã€‚  \n",
      "2. é¸æ“‡ **Go**ï¼Œä¸¦è²¼ä¸Šä»¥ä¸‹è¨­å®šï¼š\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"version\": \"0.2.0\",\n",
      "    \"configurations\": [\n",
      "        {\n",
      "            \"name\": \"Launch Package\",\n",
      "            \"type\": \"go\",\n",
      "            \"request\": \"launch\",\n",
      "            \"mode\": \"auto\",\n",
      "            \"program\": \"${fileDirname}\"\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "```\n",
      "\n",
      "> - `program` æŒ‡å‘ä½ æƒ³è¦åŸ·è¡Œçš„ Go packageï¼ˆé€šå¸¸æ˜¯ä½ ç›®å‰é–‹å•Ÿçš„æª”æ¡ˆæ‰€åœ¨ç›®éŒ„ï¼‰ã€‚  \n",
      "> - `mode: \"auto\"` æœƒè‡ªå‹•æ±ºå®šæ˜¯ `debug` é‚„æ˜¯ `test`ã€‚\n",
      "\n",
      "---\n",
      "\n",
      "## 3ï¸âƒ£ é€²è¡Œ Debug åŸ·è¡Œ\n",
      "\n",
      "1. åœ¨ VSCode å·¦å´ã€ŒRunã€é¢æ¿ï¼Œé¸æ“‡ **Launch Package**ã€‚  \n",
      "2. é»æ“Šç¶ è‰² â–¶ï¸ æŒ‰éˆ•ï¼ŒVSCode æœƒåœ¨å®¹å™¨å…§å•Ÿå‹• `go run`ï¼Œä¸¦é€²å…¥ debug æ¨¡å¼ã€‚  \n",
      "3. ä½ å¯ä»¥åœ¨ç¨‹å¼ç¢¼ä¸­è¨­ç½®æ–·é»ã€è§€å¯Ÿè®Šæ•¸ã€æ­¥é€²åŸ·è¡Œç­‰ã€‚\n",
      "\n",
      "---\n",
      "\n",
      "## 4ï¸âƒ£ å¸¸è¦‹éŒ¯èª¤èˆ‡è§£æ±ºæ–¹æ¡ˆ\n",
      "\n",
      "| éŒ¯èª¤è¨Šæ¯ | å¯èƒ½åŸå›  | è§£æ±ºæ–¹æ³• |\n",
      "|----------|----------|----------|\n",
      "| `error: Invalid userlane-proxy-path: userland-proxy is enabled, but userland-proxy-path is not set` | Docker daemon çš„ `userland-proxy` è¨­å®šä¸æ­£ç¢º | åœ¨å®¹å™¨å…§ä¿®æ”¹ `/etc/docker/daemon.json`ï¼ŒåŠ å…¥ `userland-proxy: false`ï¼Œæˆ–ç¢ºä¿ `userland-proxy-path` å·²è¨­å®šã€‚ |\n",
      "| `Running modprobe bridge br_netfilter failed with message: \" error=\"exec: \\\"modprobe\\\": executable file not found in $PATH` | `modprobe` æœªå®‰è£æˆ–ä¸åœ¨ `$PATH` | åœ¨å®¹å™¨å…§åŸ·è¡Œï¼š<br>`apt remove iptables`<br>`apt install iptables`ï¼ˆé‡æ–°å®‰è£ iptables æœƒå¸¶å…¥ modprobeï¼‰ |\n",
      "| `iptables` ç›¸é—œéŒ¯èª¤ | Docker éœ€è¦ iptables ä½†å®¹å™¨ç¼ºå°‘ | åŒä¸Šï¼šé‡æ–°å®‰è£ iptablesã€‚ |\n",
      "\n",
      "> **å‚™è¨»**ï¼šå¦‚æœä½ åœ¨å®¹å™¨å…§åŸ·è¡Œ `docker run hello-world` ä»ç„¶å¤±æ•—ï¼Œè«‹ç¢ºèªå®¹å™¨å…§å·²å®‰è£ `iptables` ä¸¦ä¸” `dockerd` æ­£åœ¨åŸ·è¡Œã€‚\n",
      "\n",
      "---\n",
      "\n",
      "## 5ï¸âƒ£ é€²éšï¼šåœ¨ Dev Container å…§è·‘ Moby\n",
      "\n",
      "å¦‚æœä½ æƒ³åœ¨ Dev Container å…§ç›´æ¥è·‘ Mobyï¼ˆè€Œä¸æ˜¯å–®ç´” debug Go ç¨‹å¼ï¼‰ï¼Œå¯ä»¥åƒè€ƒä»¥ä¸‹æµç¨‹ï¼ˆä¾†è‡ª `set-up-dev-env.md`ï¼‰ï¼š\n",
      "\n",
      "1. **åœ¨å®¿ä¸»æ©Ÿï¼ˆVMï¼‰å®‰è£ Docker**ã€‚  \n",
      "2. **åœ¨å®¿ä¸»æ©Ÿ clone å°ˆæ¡ˆä¸¦å»ºç«‹å®¹å™¨**ï¼š\n",
      "\n",
      "   ```bash\n",
      "   git clone https://github.com/moby/moby\n",
      "   cd moby\n",
      "   git checkout -b dry-run-test\n",
      "   make BIND_DIR=. shell\n",
      "   ```\n",
      "\n",
      "3. **åœ¨å®¹å™¨å…§é‡æ–°å®‰è£ iptablesï¼Œä¸¦å•Ÿå‹• dockerd**ï¼š\n",
      "\n",
      "   ```bash\n",
      "   apt remove iptables\n",
      "   apt install iptables\n",
      "   hack/make.sh binary\n",
      "   make install\n",
      "   dockerd -D\n",
      "   ```\n",
      "\n",
      "4. **åœ¨å®¿ä¸»æ©Ÿæ¸¬è©¦**ï¼š\n",
      "\n",
      "   ```bash\n",
      "   docker run hello-world\n",
      "   # Hello from Docker!\n",
      "   ```\n",
      "\n",
      "> é€™æ¨£ä½ å°±èƒ½åœ¨ Dev Container å…§å®Œæ•´è·‘ Mobyï¼Œä¸¦å¯åœ¨ VSCode å…§é€²è¡Œ debugã€‚\n",
      "\n",
      "---\n",
      "\n",
      "## 6ï¸âƒ£ å°çµ\n",
      "\n",
      "1. å®‰è£ VSCode + Dev Containersã€‚  \n",
      "2. åœ¨å®¹å™¨å…§æ‰“é–‹ VSCodeï¼Œè¨­å®š `launch.json`ã€‚  \n",
      "3. åŸ·è¡Œ `Launch Package` é€²å…¥ debugã€‚  \n",
      "4. è‹¥é‡åˆ° `userland-proxy` æˆ– `modprobe` çš„éŒ¯èª¤ï¼Œé‡æ–°å®‰è£ iptablesã€‚  \n",
      "5. å¦‚éœ€å®Œæ•´è·‘ Mobyï¼Œåƒç…§ Dev Container çš„æ­¥é©Ÿã€‚\n",
      "\n",
      "åªè¦æŒ‰ä¸Šè¿°é †åºæ“ä½œï¼Œæ‡‰è©²å°±èƒ½é †åˆ©åœ¨ VSCode å…§ä»¥ debug mode åŸ·è¡Œ Moby Projectã€‚ç¥é–‹ç™¼é †åˆ© ğŸš€\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You >  what is moby project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================route_question\n",
      "content='DevEnvAgent' additional_kwargs={} response_metadata={'model': 'gpt-oss:20b', 'created_at': '2025-09-14T13:51:52.062943721Z', 'done': True, 'done_reason': 'stop', 'total_duration': 5716286111, 'load_duration': 2411111516, 'prompt_eval_count': 246, 'prompt_eval_duration': 256729838, 'eval_count': 192, 'eval_duration': 3012339197, 'model_name': 'gpt-oss:20b'} id='run--6f85672c-a19e-4676-afbb-fc7c6458eed7-0' usage_metadata={'input_tokens': 246, 'output_tokens': 192, 'total_tokens': 438}\n",
      "=================route_question\n",
      "DevEnvAgent\n",
      "[Router â†’ DevEnvAgent]\n",
      "**Moby Project**  \n",
      "The Moby Project is the openâ€‘source foundation that Docker is built on.  \n",
      "Its source code and contribution guidelines are hosted on GitHub:\n",
      "\n",
      "- Repository: <https://github.com/moby/moby>  \n",
      "- Contribution docs: <https://github.com/moby/moby/tree/master/docs/contributing>  \n",
      "\n",
      "The project contains the core components that make up Docker, and developers can contribute to it following the coding style and guidelines listed in the repository.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You >  æ€éº¼å®‰è£moby\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================route_question\n",
      "content='DevEnvAgent' additional_kwargs={} response_metadata={'model': 'gpt-oss:20b', 'created_at': '2025-09-14T13:53:05.50495072Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1646232821, 'load_duration': 84316729, 'prompt_eval_count': 247, 'prompt_eval_duration': 123941589, 'eval_count': 92, 'eval_duration': 1435741526, 'model_name': 'gpt-oss:20b'} id='run--a4b23224-cced-453f-a224-ad63ceb40eeb-0' usage_metadata={'input_tokens': 247, 'output_tokens': 92, 'total_tokens': 339}\n",
      "=================route_question\n",
      "DevEnvAgent\n",
      "[Router â†’ DevEnvAgent]\n",
      "## å¦‚ä½•å®‰è£ Moby\n",
      "\n",
      "ä»¥ä¸‹åˆ—å‡ºæœ€å¸¸ç”¨çš„å®‰è£æ–¹å¼ï¼Œåƒ…ä¾ç…§ä½ æä¾›çš„æ–‡ä»¶å…§å®¹ã€‚\n",
      "\n",
      "---\n",
      "\n",
      "### 1. ç›´æ¥åœ¨å®¿ä¸»æ©Ÿï¼ˆVMï¼‰ä¸Šå®‰è£\n",
      "\n",
      "1. **å®‰è£ Docker**  \n",
      "   åœ¨å®¿ä¸»æ©Ÿï¼ˆä¾‹å¦‚ Rocky Linux 8ï¼‰ä¸Šå…ˆå®‰è£ Dockerï¼Œé€™æ˜¯åŸ·è¡Œ Moby çš„å‰ç½®æ¢ä»¶ã€‚  \n",
      "   ```bash\n",
      "   # ä»¥ Rocky Linux 8 ç‚ºä¾‹\n",
      "   sudo dnf install -y docker\n",
      "   sudo systemctl enable --now docker\n",
      "   ```\n",
      "\n",
      "2. **å…‹éš† Moby å°ˆæ¡ˆ**  \n",
      "   ```bash\n",
      "   git clone https://github.com/moby/moby\n",
      "   cd moby\n",
      "   ```\n",
      "\n",
      "3. **åˆ‡æ›åˆ°ä½ æƒ³è¦çš„åˆ†æ”¯æˆ–å»ºç«‹æ–°åˆ†æ”¯**  \n",
      "   ```bash\n",
      "   git checkout -b dry-run-test   # ä¾‹å¦‚å»ºç«‹ä¸€å€‹æ¸¬è©¦åˆ†æ”¯\n",
      "   ```\n",
      "\n",
      "4. **ç·¨è­¯ä¸¦å®‰è£ Moby**  \n",
      "   ```bash\n",
      "   # å…ˆç·¨è­¯äºŒé€²ä½æª”\n",
      "   hack/make.sh binary\n",
      "   # å®‰è£åˆ°ç³»çµ±\n",
      "   make install\n",
      "   ```\n",
      "\n",
      "5. **å•Ÿå‹• Docker Daemonï¼ˆdockerdï¼‰**  \n",
      "   ```bash\n",
      "   dockerd -D\n",
      "   ```\n",
      "\n",
      "6. **æ¸¬è©¦**  \n",
      "   ```bash\n",
      "   docker run hello-world\n",
      "   # ä½ æ‡‰è©²æœƒçœ‹åˆ° \"Hello from Docker!\" çš„è¨Šæ¯\n",
      "   ```\n",
      "\n",
      "---\n",
      "\n",
      "### 2. åœ¨ VSCode Dev Container å…§éƒ¨å®‰è£\n",
      "\n",
      "1. **å®‰è£ VSCode æ’ä»¶**  \n",
      "   - Dev Containers\n",
      "\n",
      "2. **é‡å•Ÿ VSCode**  \n",
      "   - æœƒé¡¯ç¤º container bash\n",
      "\n",
      "3. **é‡æ–°é–‹å•Ÿå®¹å™¨**  \n",
      "   - VSCode æŒ‡ä»¤ï¼š`Reopen in Container`\n",
      "\n",
      "4. **åœ¨å®¹å™¨å…§éƒ¨åŸ·è¡Œå®‰è£æ­¥é©Ÿ**  \n",
      "   ```bash\n",
      "   git clone https://github.com/moby/moby\n",
      "   cd moby\n",
      "   git checkout -b dry-run-test\n",
      "   make BIND_DIR=. shell   # é€²å…¥ shell\n",
      "   # ä¹‹å¾Œåœ¨ shell è£¡åŸ·è¡Œ\n",
      "   hack/make.sh binary\n",
      "   make install\n",
      "   dockerd -D\n",
      "   ```\n",
      "\n",
      "5. **æ¸¬è©¦**  \n",
      "   ```bash\n",
      "   docker run hello-world\n",
      "   ```\n",
      "\n",
      "---\n",
      "\n",
      "### 3. éœ€è¦é‡æ–°å®‰è£ iptablesï¼ˆè‹¥é‡åˆ°ç›¸é—œéŒ¯èª¤ï¼‰\n",
      "\n",
      "```bash\n",
      "sudo apt remove iptables\n",
      "sudo apt install iptables\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### 4. é€²éšï¼šåœ¨ VSCode å…§éƒ¨ä»¥ Debug æ¨¡å¼åŸ·è¡Œ Moby\n",
      "\n",
      "1. **è¨­å®š `launch.json`**  \n",
      "   ```json\n",
      "   {\n",
      "       \"version\": \"0.2.0\",\n",
      "       \"configurations\": [\n",
      "           {\n",
      "               \"name\": \"Launch Package\",\n",
      "               \"type\": \"go\",\n",
      "               \"request\": \"launch\",\n",
      "               \"mode\": \"auto\",\n",
      "               \"program\": \"${fileDirname}\"\n",
      "           }\n",
      "       ]\n",
      "   }\n",
      "   ```\n",
      "\n",
      "2. **åŸ·è¡Œ `docker.go`**  \n",
      "   - åœ¨ VSCode å…§éƒ¨ä»¥ Debug æ¨¡å¼åŸ·è¡Œ `docker.go`ã€‚\n",
      "\n",
      "---\n",
      "\n",
      "> **å‚™è¨»**  \n",
      "> - è‹¥åœ¨å®¹å™¨å…§éƒ¨åŸ·è¡Œ `dockerd` æ™‚é‡åˆ° `error: Invalid userlane-proxy-path` æˆ– `Running modprobe bridge br_netfilter failed`ï¼Œè«‹åƒè€ƒæ–‡ä»¶ä¸­æä¾›çš„ `daemon.json` ç¯„ä¾‹æˆ–é‡æ–°å®‰è£ iptablesã€‚  \n",
      "> - è‹¥è¦åœ¨ Rocky Linux 8 VM ä¸Šä½¿ç”¨ Docker Swarmï¼Œè«‹å…ˆåœç”¨ `firewalld`ï¼š  \n",
      ">   ```bash\n",
      ">   systemctl stop firewalld\n",
      ">   systemctl disable firewalld\n",
      ">   ```\n",
      "\n",
      "é€™äº›æ­¥é©Ÿå³å¯å®Œæˆ Moby çš„å®‰è£èˆ‡åŸºæœ¬æ¸¬è©¦ã€‚ç¥ä½ é–‹ç™¼é †åˆ©ï¼\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "from typing import TypedDict, Dict, Any\n",
    "from pathlib import Path\n",
    "\n",
    "# LangChain - Ollama\n",
    "try:\n",
    "    from langchain_ollama import ChatOllama\n",
    "except ImportError:\n",
    "    from langchain_community.chat_models import ChatOllama  # èˆŠç‰ˆç”¨é€™å€‹\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "# LangGraph\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# è¼‰å…¥æœ¬åœ° Prompt\n",
    "# ------------------------\n",
    "PROMPT_FILES = {\n",
    "    \"DevEnvAgent\": Path(\"DevEnvAgent.txt\"),\n",
    "    \"IssueAgent\": Path(\"IssueAgent.txt\"),\n",
    "    \"CodeTraceAgent\": Path(\"CodeTraceAgent.txt\"),\n",
    "    \"TestingAgent\": Path(\"TestingAgent.txt\"),\n",
    "}\n",
    "\n",
    "PROMPTS: Dict[str, str] = {k: v.read_text(encoding=\"utf-8\") for k, v in PROMPT_FILES.items()}\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# LLM (Ollama)\n",
    "# ------------------------\n",
    "llm = ChatOllama(\n",
    "    base_url=\"http://10.1.1.59:11434\",   # ç¢ºä¿ ollama æœå‹™åœ¨æœ¬æ©Ÿ 11434\n",
    "    model=\"gpt-oss:20b\",   # æ›æˆä½ æœ¬åœ°å¯ç”¨çš„æ¨¡å‹\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Agent å·¥å» \n",
    "# ------------------------\n",
    "def make_agent(system_prompt: str):\n",
    "    # ç”¨ f-string æŠŠæ–‡ä»¶å…§å®¹å¡é€² systemï¼Œä¸è®“ LangChain è§£æè£¡é¢çš„ {}\n",
    "    template = \"\"\"{system_prompt}\n",
    "\n",
    "ä½¿ç”¨è€…çš„å•é¡Œå¦‚ä¸‹ï¼š\n",
    "{user_input}\n",
    "\"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    chain = prompt.partial(system_prompt=system_prompt.strip()) | llm | StrOutputParser()\n",
    "    return chain\n",
    "\n",
    "AGENTS = {name: make_agent(prompt) for name, prompt in PROMPTS.items()}\n",
    "\n",
    "\n",
    "# RouterAgent Prompt\n",
    "ROUTER_PROMPT = \"\"\"\n",
    "ä½ æ˜¯ä¸€å€‹å•é¡Œåˆ†æµ Agentã€‚  \n",
    "ä½ çš„ä»»å‹™æ˜¯æ ¹æ“šä½¿ç”¨è€…çš„è¼¸å…¥ï¼Œåˆ¤æ–·æ‡‰è©²äº¤çµ¦å“ªå€‹å°ˆå®¶ Agent è™•ç†ã€‚  \n",
    "è«‹åªè¼¸å‡ºå°æ‡‰çš„ Agent åç¨±ï¼Œä¸è¦è¼¸å‡ºå¤šé¤˜æ–‡å­—ã€‚  \n",
    "\n",
    "å¯é¸çš„ Agent é¡åˆ¥æœ‰ï¼š\n",
    "- DevEnvAgent â†’ é–‹ç™¼ç’°å¢ƒã€VSCodeã€Dev Containerã€iptablesã€dockerdã€debug mode\n",
    "- IssueAgent â†’ GitHub Issueã€Swarm å•é¡Œã€éŒ¯èª¤è¨Šæ¯è¨ºæ–·\n",
    "- CodeTraceAgent â†’ Code Traceã€Container å•Ÿå‹•é‚è¼¯ã€containerdã€gRPC\n",
    "- TestingAgent â†’ æ¸¬è©¦ã€unit testã€integration testã€TESTFLAGSã€bridge_test.go\n",
    "\n",
    "è¼¸å‡ºæ ¼å¼ï¼š\n",
    "<AgentName>\n",
    "\"\"\"\n",
    "\n",
    "# å»ºç«‹ Prompt æ¨¡æ¿\n",
    "router_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", ROUTER_PROMPT),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "router_chain = router_prompt | llm\n",
    "\n",
    "def route_question(user_question: str) -> str:\n",
    "    \"\"\"å‘¼å« LLM ä¾†åˆ¤æ–·è¦åˆ†æ´¾çš„ Agent\"\"\"\n",
    "    response = router_chain.invoke({\"question\": user_question})\n",
    "    output = response.content\n",
    "    print(\"=================route_question\")\n",
    "    print(response)\n",
    "    print(\"=================route_question\")\n",
    "    return output\n",
    " \n",
    "\n",
    "# ------------------------\n",
    "# LangGraph ç‹€æ…‹å®šç¾©\n",
    "# ------------------------\n",
    "class GraphState(TypedDict):\n",
    "    user_input: str\n",
    "    routed: str | None\n",
    "    response: str\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Graph ç¯€é»\n",
    "# ------------------------\n",
    "def router_node(state: GraphState) -> GraphState:\n",
    "    agent_name = route_question(state[\"user_input\"])\n",
    "    print(agent_name)\n",
    "    return {**state, \"routed\": agent_name}\n",
    "\n",
    "def agent_node(state: GraphState) -> GraphState:\n",
    "    agent_name = state[\"routed\"]\n",
    "    chain = AGENTS[agent_name]\n",
    "    output = chain.invoke({\"user_input\": state[\"user_input\"]})\n",
    "    return {**state, \"response\": output}\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# å»ºç«‹ Graph\n",
    "# ------------------------\n",
    "graph = StateGraph(GraphState)\n",
    "graph.add_node(\"router\", router_node)\n",
    "graph.add_node(\"agent\", agent_node)\n",
    "\n",
    "graph.set_entry_point(\"router\")\n",
    "graph.add_edge(\"router\", \"agent\")\n",
    "graph.add_edge(\"agent\", END)\n",
    "\n",
    "app = graph.compile()\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# æ¸¬è©¦ç”¨å‡½æ•¸\n",
    "# ------------------------\n",
    "def run_once(user_text: str) -> Dict[str, Any]:\n",
    "    state = {\"user_input\": user_text, \"routed\": None, \"response\": \"\"}\n",
    "    result = app.invoke(state)\n",
    "    return {\n",
    "        \"routed_agent\": result[\"routed\"],\n",
    "        \"response\": result[\"response\"]\n",
    "    }\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# CLI äº’å‹•\n",
    "# ------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== RouterAgent + 4 Agents (Ollama) ===\")\n",
    "    while True:\n",
    "        q = input(\"\\nYou > \").strip()\n",
    "        if q.lower() in {\"exit\", \"quit\"}:\n",
    "            break\n",
    "        out = run_once(q)\n",
    "        print(f\"[Router â†’ {out['routed_agent']}]\")\n",
    "        print(out[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad13824e-7e09-40f3-bac3-b324b9ad254d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
